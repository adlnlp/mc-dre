{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AAvodBPXYyw"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-0YPS20QXdEo"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "train_data = json.load(open(\"Data/train_ins_gs.json\"))\n",
    "dev_data = json.load(open(\"Data/valid_ins_gs.json\"))\n",
    "test_data = json.load(open(\"Data/test_ins_gs.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Jwhbx5HBcm9K"
   },
   "outputs": [],
   "source": [
    "train_seq = train_data['sentence']\n",
    "dev_seq = dev_data['sentence']\n",
    "test_seq = test_data['sentence']\n",
    "train_slot = train_data['gt']\n",
    "dev_slot = dev_data['gt']\n",
    "test_slot = test_data['gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20y8gap-RUrg"
   },
   "source": [
    "##POS/NER(nltk, Spacy, Flair) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z0G5vwRhRbW9",
    "outputId": "26f11b1c-7466-4677-997d-5a6c8fdfe15d"
   },
   "outputs": [],
   "source": [
    "#obtain POS tag\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp = spacy.load(\"en_ner_craft_md\")\n",
    "# nlp = spacy.load(\"en_ner_jnlpba_md\")\n",
    "# nlp = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "# nlp = spacy.load(\"en_ner_bionlp13cg_md\")\n",
    "# tagger = SequenceTagger.load('ner')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"words\")\n",
    "# nltk.download(\"maxent_ne_chunker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "n3e8mUD2RoZ8"
   },
   "outputs": [],
   "source": [
    "#four functions, choose one of them\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "# nlp = en_core_web_sm.load()\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "def tag_ner_spacy(train_sentences, dev_sentences, test_sentences):\n",
    "      spacy_ner_train = []\n",
    "      for sent in train_sentences:\n",
    "          doc = nlp(\" \".join(sent))\n",
    "          ents = [(e.text, e.start, e.end, e.label_) for e in doc.ents]\n",
    "          labels = ['O'] * len(sent)\n",
    "          for idx, e in enumerate(doc.ents):\n",
    "            for i in range(e.start, e.end):\n",
    "              try:\n",
    "                labels[i] = labels[i].replace('O', e.label_)\n",
    "              except:\n",
    "                print(\"Training:\", sent)\n",
    "                print(\"*****\")\n",
    "          spacy_ner_train.append(labels)\n",
    "\n",
    "      spacy_ner_dev = []\n",
    "      for sent in dev_sentences:\n",
    "          doc = nlp(\" \".join(sent))\n",
    "          ents = [(e.text, e.start, e.end, e.label_) for e in doc.ents]\n",
    "          labels = ['O'] * len(sent)\n",
    "          for idx, e in enumerate(doc.ents):\n",
    "            for i in range(e.start, e.end):\n",
    "              try:\n",
    "                labels[i] = labels[i].replace('O', e.label_)\n",
    "              except:\n",
    "                print(\"Dev:\", sent)\n",
    "                print(\"*****\")\n",
    "          spacy_ner_dev.append(labels)\n",
    "      \n",
    "      spacy_ner_test = []\n",
    "      for sent in test_sentences:\n",
    "          doc = nlp(\" \".join(sent))\n",
    "          ents = [(e.text, e.start, e.end, e.label_) for e in doc.ents]\n",
    "          labels = ['O'] * len(sent)\n",
    "          for idx, e in enumerate(doc.ents):\n",
    "            for i in range(e.start, e.end):\n",
    "              try:\n",
    "                labels[i] = labels[i].replace('O', e.label_)\n",
    "              except:\n",
    "                print(\"Test:\", sent)\n",
    "                print(\"*****\")\n",
    "          spacy_ner_test.append(labels)\n",
    "      return spacy_ner_train, spacy_ner_dev, spacy_ner_test\n",
    "\n",
    "def tag_ner_nltk(train_sentences, dev_sentences, test_sentences):\n",
    "      ner_num = 0\n",
    "      total_num = 0\n",
    "      ner_tagger_sent_train = []\n",
    "      for sent in train_sentences:\n",
    "          temp_train = []\n",
    "          for idx, i in enumerate(nltk.chunk.ne_chunk(pos_tag(sent))):\n",
    "            total_num += 1\n",
    "            if isinstance(i, tuple):\n",
    "              temp_train.append('O')\n",
    "            else:\n",
    "              ner_num += 1\n",
    "              temp_train.extend([i.label()]*len(i))\n",
    "          ner_tagger_sent_train.append(temp_train)\n",
    "\n",
    "      ner_tagger_sent_dev = []\n",
    "      for sent in dev_sentences:\n",
    "          temp_dev = []\n",
    "          for idx, i in enumerate(nltk.chunk.ne_chunk(pos_tag(sent))):\n",
    "            total_num += 1\n",
    "            if isinstance(i, tuple):\n",
    "              temp_dev.append('O')\n",
    "            else:\n",
    "              ner_num += 1\n",
    "              temp_dev.extend([i.label()]*len(i))\n",
    "          ner_tagger_sent_dev.append(temp_dev)\n",
    "\n",
    "      ner_tagger_sent_test = []\n",
    "      for sent in test_sentences:\n",
    "          temp_test = []\n",
    "          for idx, i in enumerate(nltk.chunk.ne_chunk(pos_tag(sent))):\n",
    "            total_num += 1\n",
    "            if isinstance(i, tuple):\n",
    "              temp_test.append('O')\n",
    "            else:\n",
    "              ner_num += 1\n",
    "              temp_test.extend([i.label()]*len(i))\n",
    "          ner_tagger_sent_test.append(temp_test)\n",
    "      print(\"ner_num\", ner_num)\n",
    "      print(\"total_num\", total_num)\n",
    "      return ner_tagger_sent_train, ner_tagger_sent_dev, ner_tagger_sent_test\n",
    "\n",
    "def tag_ner_flair(train_sentences, dev_sentences, test_sentences):\n",
    "      flair_ner_train = []\n",
    "      for sent in train_sentences:\n",
    "        sentence = Sentence(\" \".join(sent))\n",
    "        tagger.predict(sentence)\n",
    "        labels = ['O'] * len(sent)\n",
    "        for label in sentence.labels:\n",
    "          m = str(label)\n",
    "          a = m[m.find(\"[\")+1:m.find(\"]\")]\n",
    "          try:\n",
    "            for i in range(int(a.split(\":\")[0]), int(a.split(\":\")[1])):\n",
    "                labels[i] = labels[i].replace('O', label.value)\n",
    "          except:\n",
    "            print(\"Training:\", sent)\n",
    "            print(\"*****\")\n",
    "        flair_ner_train.append(labels)\n",
    "      \n",
    "      flair_ner_dev = []\n",
    "      for sent in dev_sentences:\n",
    "        sentence = Sentence(\" \".join(sent))\n",
    "        tagger.predict(sentence)\n",
    "        labels = ['O'] * len(sent)\n",
    "        for label in sentence.labels:\n",
    "          m = str(label)\n",
    "          a = m[m.find(\"[\")+1:m.find(\"]\")]\n",
    "          try:\n",
    "            for i in range(int(a.split(\":\")[0]), int(a.split(\":\")[1])):\n",
    "                labels[i] = labels[i].replace('O', label.value)\n",
    "          except:\n",
    "            print(\"Dev:\", sent)\n",
    "            print(\"*****\")\n",
    "        flair_ner_dev.append(labels)\n",
    "      \n",
    "      flair_ner_test = []\n",
    "      for sent in test_sentences:\n",
    "        sentence = Sentence(\" \".join(sent))\n",
    "        tagger.predict(sentence)\n",
    "        labels = ['O'] * len(sent)\n",
    "        for label in sentence.labels:\n",
    "          m = str(label)\n",
    "          a = m[m.find(\"[\")+1:m.find(\"]\")]\n",
    "          try:\n",
    "            for i in range(int(a.split(\":\")[0]), int(a.split(\":\")[1])):\n",
    "                labels[i] = labels[i].replace('O', label.value)\n",
    "          except:\n",
    "            print(\"Test:\", sent)\n",
    "            print(\"*****\")\n",
    "        flair_ner_test.append(labels)\n",
    "      return flair_ner_train, flair_ner_dev, flair_ner_test\n",
    "\n",
    "def tag_pos(train_sentences, dev_sentences, test_sentences):\n",
    "      pos_tagged_sent = []\n",
    "      pos_tagged_sent_train = []\n",
    "      for sent in train_sentences:\n",
    "          pos_tagged_sent.extend(pos_tag(sent))\n",
    "          pos_tagged_sent_train.append(pos_tag(sent))\n",
    "\n",
    "      pos_tagged_sent_dev = []\n",
    "      for sent in dev_sentences:\n",
    "          pos_tagged_sent_dev.append(pos_tag(sent))\n",
    "\n",
    "      pos_tagged_sent_test = []\n",
    "      for sent in test_sentences:\n",
    "          pos_tagged_sent_test.append(pos_tag(sent))\n",
    "\n",
    "      tags = list(set([i[1] for i in pos_tagged_sent]))\n",
    "\n",
    "      pos_input_dim = len(tags)\n",
    "      char_to_int = dict((c, i) for i, c in enumerate(tags))\n",
    "      int_to_char = dict((i, c) for i, c in enumerate(tags))\n",
    "\n",
    "      train_pos_encoded =[]\n",
    "      for i in range(len(pos_tagged_sent_train)):\n",
    "          temp = [pos[1] for pos in pos_tagged_sent_train[i]]\n",
    "          train_pos_encoded.append(temp)\n",
    "      \n",
    "      dev_pos_encoded =[]\n",
    "      for i in range(len(pos_tagged_sent_dev)):\n",
    "          temp = [pos[1] for pos in pos_tagged_sent_dev[i]]\n",
    "          dev_pos_encoded.append(temp)\n",
    "\n",
    "      test_pos_encoded =[]\n",
    "      for i in range(len(pos_tagged_sent_test)):\n",
    "          temp = [pos[1] for pos in pos_tagged_sent_test[i]]\n",
    "          test_pos_encoded.append(temp)\n",
    "\n",
    "      return train_pos_encoded, dev_pos_encoded, test_pos_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tk7eB9LhSChv"
   },
   "outputs": [],
   "source": [
    "train_seq_pos, dev_seq_pos, test_seq_pos = tag_pos(train_seq, dev_seq, test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMnaWSrLSLVG",
    "outputId": "c958718b-34fa-4b53-a057-b515a42b442c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "Training data: 41496\n",
      "Development data: 4538\n",
      "Testing data: 30614\n",
      "####################################################################################################\n",
      "Data example:\n",
      "Sequence: \n",
      "['She', 'had', 'fevers', ',', 'weakness', ',', 'and', 'diarrhea', '.']\n",
      "\n",
      "Slots: \n",
      "['O', 'O', 'B-ADE', 'O', 'B-ADE', 'O', 'O', 'B-ADE', 'O']\n",
      "\n",
      "POS: \n",
      "['PRP', 'VBD', 'NNS', ',', 'NN', ',', 'CC', 'NN', '.']\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "print(\"#\"*100)\n",
    "print(\"Training data: \"+str(len(train_data['sentence'])))\n",
    "print(\"Development data: \"+str(len(dev_data['sentence'])))\n",
    "print(\"Testing data: \"+str(len(test_data['sentence'])))\n",
    "print(\"#\"*100)\n",
    "print(\"Data example:\")\n",
    "print(\"Sequence: \")\n",
    "print(test_seq[5])\n",
    "print()\n",
    "print(\"Slots: \")\n",
    "print(test_slot[5])\n",
    "print()\n",
    "print(\"POS: \")\n",
    "print(test_seq_pos[5])\n",
    "print(\"#\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ojJAX305SPy9",
    "outputId": "bd17cc86-ab89-4498-b8cf-e9bc4fca1d66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################################################\n",
      "Total slots: {'I-Reason', 'B-Reason', '<PAD>', 'I-Dosage', 'B-Duration', 'B-Dosage', 'I-Route', 'O', 'B-Drug', 'B-Route', 'B-Strength', 'I-Form', '<UNK>', 'I-Duration', 'I-ADE', 'I-Frequency', 'B-Form', 'I-Strength', 'B-ADE', 'I-Drug', 'B-Frequency'}\n",
      "Total pos: {'VBZ', 'WDT', 'VBD', 'RBS', '<PAD>', 'FW', '.', 'SYM', 'DT', 'PDT', 'VBP', 'TO', ':', 'NN', 'WP$', 'UH', 'PRP$', ',', 'VBG', 'MD', '$', 'LS', 'WRB', 'JJR', 'IN', 'CD', ')', '<UNK>', 'CC', 'RBR', 'VBN', 'PRP', 'NNPS', 'VB', '(', 'NNP', 'RB', \"''\", 'JJ', 'RP', '#', 'NNS', 'JJS', 'EX', 'POS', 'WP'}\n",
      "#####################################################################################\n",
      "#####################################################################################\n",
      "Total slots type: 21\n",
      "Total pos type: 46\n",
      "#####################################################################################\n",
      "['B-Form', 'I-Strength', 'B-ADE', 'I-Drug', 'B-Frequency']\n",
      "['NNS', 'JJS', 'EX', 'POS', 'WP']\n",
      "#####################################################################################\n"
     ]
    }
   ],
   "source": [
    "slot_all=set()\n",
    "pos_all=set()\n",
    "\n",
    "\n",
    "for train_sequ, train_int in zip(train_slot, train_seq_pos):\n",
    "  for i in train_sequ:\n",
    "    slot_all.add(i)\n",
    "  for i in train_int:\n",
    "    pos_all.add(i)\n",
    "\n",
    "#temp=['<PAD>']\n",
    "#temp.extend(list(slot_all))\n",
    "slot_all.add('<PAD>')\n",
    "slot_all.add('<UNK>')\n",
    "# slot_all.add('[CLS]')\n",
    "pos_all.add('<PAD>')\n",
    "pos_all.add('<UNK>')\n",
    "# pos_all=list(pos_all)\n",
    "\n",
    "slot_num=len(slot_all)\n",
    "pos_num=len(pos_all)\n",
    "\n",
    "print(\"#\"*85)\n",
    "print(\"Total slots: \" + str(slot_all))\n",
    "print(\"Total pos: \" + str(pos_all))\n",
    "\n",
    "\n",
    "print(\"#\"*85)\n",
    "\n",
    "print(\"#\"*85)\n",
    "print(\"Total slots type: \" + str(slot_num))\n",
    "print(\"Total pos type: \" + str(pos_num))\n",
    "\n",
    "\n",
    "print(\"#\"*85)\n",
    "\n",
    "print(list(slot_all)[-5:])\n",
    "print(list(pos_all)[-5:])\n",
    "\n",
    "\n",
    "print(\"#\"*85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ta_Tifv_SWvc",
    "outputId": "c8a92314-4ced-448b-e577-5b59624e3bbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<UNK>': 1, 'B-ADE': 2, 'B-Dosage': 3, 'B-Drug': 4, 'B-Duration': 5, 'B-Form': 6, 'B-Frequency': 7, 'B-Reason': 8, 'B-Route': 9, 'B-Strength': 10, 'I-ADE': 11, 'I-Dosage': 12, 'I-Drug': 13, 'I-Duration': 14, 'I-Form': 15, 'I-Frequency': 16, 'I-Reason': 17, 'I-Route': 18, 'I-Strength': 19, 'O': 20}\n",
      "{'#': 0, '$': 1, \"''\": 2, '(': 3, ')': 4, ',': 5, '.': 6, ':': 7, '<PAD>': 8, '<UNK>': 9, 'CC': 10, 'CD': 11, 'DT': 12, 'EX': 13, 'FW': 14, 'IN': 15, 'JJ': 16, 'JJR': 17, 'JJS': 18, 'LS': 19, 'MD': 20, 'NN': 21, 'NNP': 22, 'NNPS': 23, 'NNS': 24, 'PDT': 25, 'POS': 26, 'PRP': 27, 'PRP$': 28, 'RB': 29, 'RBR': 30, 'RBS': 31, 'RP': 32, 'SYM': 33, 'TO': 34, 'UH': 35, 'VB': 36, 'VBD': 37, 'VBG': 38, 'VBN': 39, 'VBP': 40, 'VBZ': 41, 'WDT': 42, 'WP': 43, 'WP$': 44, 'WRB': 45}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lEnc_slot = LabelEncoder()\n",
    "lEnc_pos = LabelEncoder()\n",
    "slots=np.array(list(slot_all))\n",
    "pos=np.array(list(pos_all))\n",
    "lEnc_slot.fit(slots)\n",
    "lEnc_pos.fit(pos)\n",
    "\n",
    "\n",
    "integer_mapping_slot = {l: i for i, l in enumerate(lEnc_slot.classes_)}\n",
    "integer_mapping_pos = {l: i for i, l in enumerate(lEnc_pos.classes_)}\n",
    "print(integer_mapping_slot)\n",
    "print(integer_mapping_pos)\n",
    "\n",
    "train_slot_transformed=[]\n",
    "dev_slot_transformed=[]\n",
    "test_slot_transformed=[]\n",
    "for seq_tr in train_slot:\n",
    "  label=[integer_mapping_slot[i] if i in integer_mapping_slot else integer_mapping_slot['<UNK>'] for i in seq_tr]\n",
    "  train_slot_transformed.append(label)\n",
    "\n",
    "for seq_de in dev_slot:\n",
    "  label=[integer_mapping_slot[i] if i in integer_mapping_slot else integer_mapping_slot['<UNK>'] for i in seq_de]\n",
    "  dev_slot_transformed.append(label)\n",
    "\n",
    "for seq_te in test_slot:\n",
    "  label=[integer_mapping_slot[i] if i in integer_mapping_slot else integer_mapping_slot['<UNK>'] for i in seq_te]\n",
    "  test_slot_transformed.append(label) \n",
    "    \n",
    "\n",
    "train_pos_transformed=[]\n",
    "dev_pos_transformed=[]\n",
    "test_pos_transformed=[]\n",
    "for seq_tr in train_seq_pos:\n",
    "  label=[integer_mapping_pos[i] if i in integer_mapping_pos else integer_mapping_pos['<UNK>'] for i in seq_tr]\n",
    "  train_pos_transformed.append(label)\n",
    "\n",
    "for seq_de in dev_seq_pos:\n",
    "  label=[integer_mapping_pos[i] if i in integer_mapping_pos else integer_mapping_pos['<UNK>'] for i in seq_de]\n",
    "  dev_pos_transformed.append(label)\n",
    "\n",
    "for seq_te in test_seq_pos:\n",
    "  label=[integer_mapping_pos[i] if i in integer_mapping_pos else integer_mapping_pos['<UNK>'] for i in seq_te]\n",
    "  test_pos_transformed.append(label) \n",
    "  \n",
    "# train_pos_transformed=lEnc_intent.transform(train_seq_pos)\n",
    "# dev_pos_transformed=[integer_mapping_intent[i] if i in integer_mapping_intent else integer_mapping_intent['<UNK>'] for i in dev_seq_pos]\n",
    "# test_pos_transformed=[integer_mapping_intent[i] if i in integer_mapping_intent else integer_mapping_intent['<UNK>'] for i in test_seq_pos]\n",
    " \n",
    "pad_slot_id=integer_mapping_slot['<PAD>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mxJvYe9cSb7v"
   },
   "outputs": [],
   "source": [
    "def padding_input(X_train, X_dev, X_test,maxlen):\n",
    "    \"\"\"\n",
    "        Pads the input upto considered max length\n",
    "        Args:\n",
    "            X_train = tokenized train data\n",
    "            X_test = tokenized test data\n",
    "        Returns:\n",
    "            X_train_pad = padded tokenized train data\n",
    "            X_test_pad = padded tokenized test data\n",
    "    \"\"\"\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train,maxlen=maxlen,padding=\"post\")\n",
    "\n",
    "    X_dev_pad = pad_sequences(X_dev,maxlen=maxlen,padding=\"post\")\n",
    "\n",
    "    X_test_pad = pad_sequences(X_test,maxlen=maxlen,padding=\"post\")\n",
    "\n",
    "    return X_train_pad,X_dev_pad,X_test_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3ho8Sw2DSdCB"
   },
   "outputs": [],
   "source": [
    "train_slot_padded,dev_slot_padded, test_slot_padded = padding_input(train_slot_transformed, dev_slot_transformed, test_slot_transformed, 50)\n",
    "train_pos_padded,dev_pos_padded, test_pos_padded = padding_input(train_pos_transformed, dev_pos_transformed, test_pos_transformed, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tpHBN1PMShR2",
    "outputId": "39bcd964-2bc4-4935-efca-a0d472efc240"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41496, 50)\n",
      "(4538, 50)\n",
      "(30614, 50)\n",
      "(41496, 50)\n",
      "(4538, 50)\n",
      "(30614, 50)\n"
     ]
    }
   ],
   "source": [
    "print(train_slot_padded.shape)\n",
    "print(dev_slot_padded.shape)\n",
    "print(test_slot_padded.shape)\n",
    "print(train_pos_padded.shape)\n",
    "print(dev_pos_padded.shape)\n",
    "print(test_pos_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gj0VeLDoSoyo"
   },
   "source": [
    "# Get Embedding(Glove, BERT, BioBERT, ClinicalBERT, PubmedBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-cMdtm-aSvtv",
    "outputId": "2c55cf14-4ab4-4e18-9a54-a11681fb8a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41496, 50, 300)\n",
      "(4538, 50, 300)\n",
      "(30614, 50, 300)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "f=h5py.File('Data/PUBMEDBERT/train_pubmedbert_verify.h5', 'r')\n",
    "train_seq_glove=np.array(f[list(f.keys())[0]])\n",
    "f=h5py.File('Data/PUBMEDBERT/dev_pubmedbert_verify.h5', 'r')\n",
    "dev_seq_glove=np.array(f[list(f.keys())[0]])\n",
    "f=h5py.File('Data/PUBMEDBERT/test_pubmedbert_verify.h5', 'r')\n",
    "test_seq_glove=np.array(f[list(f.keys())[0]])\n",
    "\n",
    "train_seq_glove = np.array([train_seq[:, :300] for train_seq in train_seq_glove])\n",
    "dev_seq_glove = np.array([dev_seq[:, :300] for dev_seq in dev_seq_glove])\n",
    "test_seq_glove = np.array([test_seq[:, :300] for test_seq in test_seq_glove])\n",
    "\n",
    "print(train_seq_glove.shape)\n",
    "print(dev_seq_glove.shape)\n",
    "print(test_seq_glove.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=set()\n",
    "max_len=0\n",
    "\n",
    "for seq_train in train_seq:\n",
    "  max_len=max(len(seq_train),max_len)\n",
    "  for word in seq_train:\n",
    "    vocab.add(word)\n",
    "\n",
    "temp=['<PAD>','<UNK>']\n",
    "temp.extend(list(vocab))\n",
    "vocab=temp\n",
    "VOCAB_SIZE=len(vocab)    \n",
    "MAX_SEQ_LENGTH=max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eGwJBU5x4mmC"
   },
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab2inx={v:k for k,v in enumerate(vocab)}\n",
    "train_seq_encoded=[[vocab2inx[w] for w in sent] for sent in train_seq]\n",
    "dev_seq_encoded=[[vocab2inx[w] if w in vocab2inx else vocab2inx['<UNK>'] for w in sent] for sent in dev_seq]\n",
    "test_seq_encoded=[[vocab2inx[w] if w in vocab2inx else vocab2inx['<UNK>'] for w in sent] for sent in test_seq]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "K1YVMsWy5z_j"
   },
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab2inx={v:k for k,v in enumerate(vocab)}\n",
    "train_seq_encoded=[[vocab2inx[w] for w in sent] for sent in train_seq]\n",
    "dev_seq_encoded=[[vocab2inx[w] if w in vocab2inx else vocab2inx['<UNK>'] for w in sent] for sent in dev_seq]\n",
    "test_seq_encoded=[[vocab2inx[w] if w in vocab2inx else vocab2inx['<UNK>'] for w in sent] for sent in test_seq]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "EjQkPN4F52Pi"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 50\n",
    "train_sequences_padded = pad_sequences(train_seq_encoded, maxlen=MAX_LEN, padding='post')\n",
    "dev_sequences_padded = pad_sequences(dev_seq_encoded, maxlen=MAX_LEN, padding='post')\n",
    "test_sequences_padded = pad_sequences(test_seq_encoded, maxlen=MAX_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XV0z_lWR6EiE",
    "outputId": "1fdc32e6-6258-477c-e241-c599f73fcce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41496, 50)\n",
      "(4538, 50)\n",
      "(30614, 50)\n",
      "(41496, 50)\n",
      "(4538, 50)\n",
      "(30614, 50)\n",
      "(41496, 50)\n",
      "(4538, 50)\n",
      "(30614, 50)\n"
     ]
    }
   ],
   "source": [
    "# train_slot_padded=np.array(train_slot_padded)\n",
    "# dev_slot_padded=np.array(dev_slot_padded)\n",
    "# test_slot_padded=np.array(test_slot_padded)\n",
    "# train_pos_padded=np.array(train_pos_padded)\n",
    "# dev_pos_padded=np.array(dev_pos_padded)\n",
    "# test_pos_padded=np.array(test_pos_padded)\n",
    "print(train_sequences_padded.shape)\n",
    "print(dev_sequences_padded.shape)\n",
    "print(test_sequences_padded.shape)\n",
    "print(train_slot_padded.shape)\n",
    "print(dev_slot_padded.shape)\n",
    "print(test_slot_padded.shape)\n",
    "print(train_pos_padded.shape)\n",
    "print(dev_pos_padded.shape)\n",
    "print(test_pos_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDV03U0_7Tbw",
    "outputId": "c22d65ab-b627-4881-cdcc-f8b777b8542e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "slot_num=len(integer_mapping_slot)\n",
    "pos_num=len(integer_mapping_pos)\n",
    "print(slot_num)\n",
    "print(pos_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwN0CaQ67Z5A"
   },
   "source": [
    "# Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "BQT4yjx27cWG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import keras.utils\n",
    "import keras.activations\n",
    "import keras.applications\n",
    "import keras.backend\n",
    "import keras.datasets\n",
    "import keras.engine\n",
    "import keras.layers\n",
    "import keras.preprocessing\n",
    "import keras.wrappers\n",
    "import keras.callbacks\n",
    "import keras.constraints\n",
    "import keras.initializers\n",
    "import keras.metrics\n",
    "import keras.models\n",
    "import keras.losses\n",
    "import keras.optimizers\n",
    "import keras.regularizers \n",
    "#from keras.optimizers import Adam\n",
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHStNLt17gZr",
    "outputId": "7e770253-3f36-4018-8df5-99040581bff0"
   },
   "outputs": [],
   "source": [
    "# !pip install keras-layer-normalization\n",
    "# !pip install keras_pos_embd\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras_pos_embd import TrigPosEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "QlVL1LwPIWlx"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import math \n",
    "\n",
    "seq_length = 50\n",
    "\n",
    "\n",
    "def matmul(x):\n",
    "    return tf.matmul(x[0], x[1])\n",
    "\n",
    "def split_heads(x,num_heads):\n",
    "    x = keras.layers.Reshape((seq_length, num_heads, -1))(x)\n",
    "    return keras.layers.Permute((2,1,3))(x)\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, depth):\n",
    "  matmul_qk = keras.layers.Lambda(matmul)([q,keras.layers.Permute((1,3,2))(k)]) \n",
    "  # scale matmul_qk\n",
    "  scaled_attention_logits= keras.layers.Lambda(lambda x:x/math.sqrt(depth))(matmul_qk)\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = keras.layers.Activation('softmax')(scaled_attention_logits)  # (..., seq_len_q, seq_len_k)\n",
    "  output = keras.layers.Lambda(matmul)([attention_weights, v])\n",
    "  return output\n",
    "\n",
    "\n",
    "def trans(input_emb, d_model,num_heads,dff,dropout):\n",
    "\n",
    "  depth = d_model/num_heads\n",
    "\n",
    "  q = keras.layers.Dense(d_model)(input_emb)\n",
    "  k = keras.layers.Dense(d_model)(input_emb)\n",
    "  v = keras.layers.Dense(d_model)(input_emb)\n",
    "\n",
    "  qw = split_heads(q,num_heads)  \n",
    "  kw = split_heads(k,num_heads)  \n",
    "  vw = split_heads(v,num_heads) \n",
    "\n",
    "  scaled_attention = scaled_dot_product_attention(qw, kw, vw, depth)\n",
    "  scaled_attention = keras.layers.Permute((2,1,3))(scaled_attention)  \n",
    "  concat_attention = keras.layers.Reshape((-1, d_model))(scaled_attention)\n",
    "  concat_attention = keras.layers.Dense(d_model)(concat_attention)\n",
    "  concat_attention = keras.layers.Dropout(dropout)(concat_attention)\n",
    "  out_1 = LayerNormalization()(keras.layers.Add()([q,concat_attention]))\n",
    "  ffn_output = keras.layers.Dense(d_model)(keras.layers.Dense(dff, activation='relu')(out_1))\n",
    "  ffn_output = keras.layers.Dropout(dropout)(ffn_output)\n",
    "  out_2 = LayerNormalization()(keras.layers.Add()([ffn_output,out_1]))\n",
    "\n",
    "  return out_2\n",
    "\n",
    "\n",
    "####################\n",
    "#####  noex #####\n",
    "####################\n",
    "\n",
    "\n",
    "def new_trans_noex(input_slot_pos,d_model,num_heads,dff,dropout):\n",
    "  input_slot = input_slot_pos[0]\n",
    "  input_pos = input_slot_pos[1]\n",
    "  depth = d_model/num_heads\n",
    "\n",
    "  q_slot_1 = keras.layers.Dense(d_model)(input_slot)\n",
    "  k_slot_1 = keras.layers.Dense(d_model)(input_slot)\n",
    "  v_slot_1 = keras.layers.Dense(d_model)(input_slot)\n",
    "  q_pos_1 = keras.layers.Dense(d_model)(input_pos)\n",
    "  k_pos_1 = keras.layers.Dense(d_model)(input_pos)\n",
    "  v_pos_1 = keras.layers.Dense(d_model)(input_pos)\n",
    "\n",
    "  qw_slot_1 = split_heads(q_slot_1,num_heads)  \n",
    "  kw_slot_1 = split_heads(k_slot_1,num_heads)  \n",
    "  vw_slot_1 = split_heads(v_slot_1,num_heads) \n",
    "\n",
    "  qw_pos_1 = split_heads(q_pos_1,num_heads)  \n",
    "  kw_pos_1 = split_heads(k_pos_1,num_heads)  \n",
    "  vw_pos_1 = split_heads(v_pos_1,num_heads) \n",
    "\n",
    "\n",
    "  scaled_attention_pos = scaled_dot_product_attention(qw_pos_1, kw_pos_1, vw_pos_1, depth)\n",
    "  #scaled_attention_intent = scaled_dot_product_attention(qw_intent_1, kw_slot_1, vw_slot_1, depth)\n",
    "  #scaled_attention_intent = scaled_dot_product_attention(qw_slot_1, kw_intent_1, vw_intent_1, depth)\n",
    "  scaled_attention_pos = keras.layers.Permute((2,1,3))(scaled_attention_pos)  \n",
    "  concat_attention_pos = keras.layers.Reshape((-1, d_model))(scaled_attention_pos)\n",
    "  concat_attention_pos = keras.layers.Dense(d_model)(concat_attention_pos)\n",
    "  concat_attention_pos = keras.layers.Dropout(dropout)(concat_attention_pos)\n",
    "  out_1_pos = LayerNormalization()(keras.layers.Add()([q_pos_1,concat_attention_pos]))\n",
    "  \n",
    "\n",
    "  scaled_attention_slot = scaled_dot_product_attention(qw_slot_1, kw_slot_1, vw_slot_1, depth)\n",
    "  #scaled_attention_slot = scaled_dot_product_attention(qw_slot_1, kw_intent_1, vw_intent_1, depth)\n",
    "  #scaled_attention_slot = scaled_dot_product_attention(qw_intent_1, kw_slot_1, vw_slot_1, depth)\n",
    "  scaled_attention_slot = keras.layers.Permute((2,1,3))(scaled_attention_slot)\n",
    "  concat_attention_slot = keras.layers.Reshape((-1, d_model))(scaled_attention_slot)\n",
    "  concat_attention_slot = keras.layers.Dense(d_model)(concat_attention_slot)\n",
    "  concat_attention_slot = keras.layers.Dropout(dropout)(concat_attention_slot)\n",
    "  out_1_slot = LayerNormalization()(keras.layers.Add()([q_slot_1,concat_attention_slot]))\n",
    "  \n",
    "  ffn_output_pos = keras.layers.Dense(d_model)(keras.layers.Dense(dff, activation='relu')(out_1_pos))\n",
    "  ffn_output_pos = keras.layers.Dropout(dropout)(ffn_output_pos)\n",
    "  out_2_pos = LayerNormalization()(keras.layers.Add()([ffn_output_pos,out_1_pos]))\n",
    "  \n",
    "  \n",
    "  ffn_output_slot = keras.layers.Dense(d_model)(keras.layers.Dense(dff, activation='relu')(out_1_slot))\n",
    "  ffn_output_slot = keras.layers.Dropout(dropout)(ffn_output_slot)\n",
    "  out_2_slot = LayerNormalization()(keras.layers.Add()([ffn_output_slot,out_1_slot]))\n",
    "\n",
    "  return out_2_slot,out_2_pos\n",
    "\n",
    "\n",
    "###################\n",
    "##### bf  ######\n",
    "###################\n",
    "\n",
    "\n",
    "def new_trans_bf(input_slot_pos,d_model,num_heads,dff,dropout):\n",
    "  input_slot = input_slot_pos[0]\n",
    "  input_pos = input_slot_pos[1]\n",
    "  depth = d_model/num_heads\n",
    "\n",
    "  q_slot_1 = keras.layers.Dense(d_model)(input_slot)\n",
    "  k_slot_1 = keras.layers.Dense(d_model)(input_slot)\n",
    "  v_slot_1 = keras.layers.Dense(d_model)(input_slot)\n",
    "  q_pos_1 = keras.layers.Dense(d_model)(input_pos)\n",
    "  k_pos_1 = keras.layers.Dense(d_model)(input_pos)\n",
    "  v_pos_1 = keras.layers.Dense(d_model)(input_pos)\n",
    "\n",
    "  qw_slot_1 = split_heads(q_slot_1,num_heads)  \n",
    "  kw_slot_1 = split_heads(k_slot_1,num_heads)  \n",
    "  vw_slot_1 = split_heads(v_slot_1,num_heads) \n",
    "\n",
    "  qw_pos_1 = split_heads(q_pos_1,num_heads)  \n",
    "  kw_pos_1 = split_heads(k_pos_1,num_heads)  \n",
    "  vw_pos_1 = split_heads(v_pos_1,num_heads) \n",
    "  \n",
    "\n",
    "  scaled_attention_pos = scaled_dot_product_attention(qw_pos_1, kw_pos_1, vw_pos_1, depth)\n",
    "  #scaled_attention_intent = scaled_dot_product_attention(qw_intent_1, kw_slot_1, vw_slot_1, depth)\n",
    "  #scaled_attention_intent = scaled_dot_product_attention(qw_slot_1, kw_intent_1, vw_intent_1, depth)\n",
    "  scaled_attention_pos = keras.layers.Permute((2,1,3))(scaled_attention_pos) \n",
    "  concat_attention_pos = keras.layers.Reshape((-1, d_model))(scaled_attention_pos)\n",
    "  concat_attention_pos = keras.layers.Dense(d_model)(concat_attention_pos)\n",
    "  concat_attention_pos = keras.layers.Dropout(dropout)(concat_attention_pos)\n",
    "  out_1_pos = LayerNormalization()(keras.layers.Add()([q_pos_1,concat_attention_pos]))\n",
    "\n",
    "  scaled_attention_slot = scaled_dot_product_attention(qw_slot_1, kw_slot_1, vw_slot_1, depth)\n",
    "  #scaled_attention_slot = scaled_dot_product_attention(qw_slot_1, kw_intent_1, vw_intent_1, depth)\n",
    "  #scaled_attention_slot = scaled_dot_product_attention(qw_intent_1, kw_slot_1, vw_slot_1, depth)\n",
    "  scaled_attention_slot = keras.layers.Permute((2,1,3))(scaled_attention_slot)\n",
    "  concat_attention_slot = keras.layers.Reshape((-1, d_model))(scaled_attention_slot)\n",
    "  concat_attention_slot = keras.layers.Dense(d_model)(concat_attention_slot)\n",
    "  concat_attention_slot = keras.layers.Dropout(dropout)(concat_attention_slot)\n",
    "  out_1_slot = LayerNormalization()(keras.layers.Add()([q_slot_1,concat_attention_slot]))\n",
    "  \n",
    "  ffn_output_pos = keras.layers.Dense(d_model)(keras.layers.Dense(dff, activation='relu')(out_1_slot))\n",
    "  ffn_output_pos = keras.layers.Dropout(dropout)(ffn_output_pos)\n",
    "  out_2_pos = LayerNormalization()(keras.layers.Add()([ffn_output_pos,out_1_pos]))\n",
    "  \n",
    "  \n",
    "  ffn_output_slot = keras.layers.Dense(d_model)(keras.layers.Dense(dff, activation='relu')(out_1_pos))\n",
    "  ffn_output_slot = keras.layers.Dropout(dropout)(ffn_output_slot)\n",
    "  out_2_slot = LayerNormalization()(keras.layers.Add()([ffn_output_slot,out_1_slot]))\n",
    "  return out_2_slot,out_2_pos\n",
    "\n",
    "###################\n",
    "#####  af ######\n",
    "###################\n",
    "\n",
    "\n",
    "def new_trans_af(input_slot_pos,d_model,num_heads,dff,dropout):\n",
    "  input_slot = input_slot_pos[0]\n",
    "  input_pos = input_slot_pos[1]\n",
    "  depth = d_model/num_heads\n",
    "\n",
    "  q_slot_1 = keras.layers.Dense(d_model)(input_slot)\n",
    "  k_slot_1 = keras.layers.Dense(d_model)(input_slot)\n",
    "  v_slot_1 = keras.layers.Dense(d_model)(input_slot)\n",
    "  q_pos_1 = keras.layers.Dense(d_model)(input_pos)\n",
    "  k_pos_1 = keras.layers.Dense(d_model)(input_pos)\n",
    "  v_pos_1 = keras.layers.Dense(d_model)(input_pos)\n",
    "\n",
    "  qw_slot_1 = split_heads(q_slot_1,num_heads)  \n",
    "  kw_slot_1 = split_heads(k_slot_1,num_heads)  \n",
    "  vw_slot_1 = split_heads(v_slot_1,num_heads) \n",
    "\n",
    "  qw_pos_1 = split_heads(q_pos_1,num_heads)  \n",
    "  kw_pos_1 = split_heads(k_pos_1,num_heads)  \n",
    "  vw_pos_1 = split_heads(v_pos_1,num_heads) \n",
    "\n",
    "  scaled_attention_pos = scaled_dot_product_attention(qw_pos_1, kw_pos_1, vw_pos_1, depth)\n",
    "  #scaled_attention_intent = scaled_dot_product_attention(qw_intent_1, kw_slot_1, vw_slot_1, depth)\n",
    "  #scaled_attention_intent = scaled_dot_product_attention(qw_slot_1, kw_intent_1, vw_intent_1, depth)\n",
    "  scaled_attention_pos = keras.layers.Permute((2,1,3))(scaled_attention_pos)  \n",
    "  concat_attention_pos = keras.layers.Reshape((-1, d_model))(scaled_attention_pos)\n",
    "  concat_attention_pos = keras.layers.Dense(d_model)(concat_attention_pos)\n",
    "  concat_attention_pos = keras.layers.Dropout(dropout)(concat_attention_pos)\n",
    "  out_1_pos = LayerNormalization()(keras.layers.Add()([q_pos_1,concat_attention_pos]))\n",
    "  \n",
    "\n",
    "  scaled_attention_slot = scaled_dot_product_attention(qw_slot_1, kw_slot_1, vw_slot_1, depth)\n",
    "  #scaled_attention_slot = scaled_dot_product_attention(qw_slot_1, kw_intent_1, vw_intent_1, depth)\n",
    "  #scaled_attention_slot = scaled_dot_product_attention(qw_intent_1, kw_slot_1, vw_slot_1, depth)\n",
    "  scaled_attention_slot = keras.layers.Permute((2,1,3))(scaled_attention_slot)\n",
    "  concat_attention_slot = keras.layers.Reshape((-1, d_model))(scaled_attention_slot)\n",
    "  concat_attention_slot = keras.layers.Dense(d_model)(concat_attention_slot)\n",
    "  concat_attention_slot = keras.layers.Dropout(dropout)(concat_attention_slot)\n",
    "  out_1_slot = LayerNormalization()(keras.layers.Add()([q_slot_1,concat_attention_slot]))\n",
    "  \n",
    "  ffn_output_pos = keras.layers.Dense(d_model)(keras.layers.Dense(dff, activation='relu')(out_1_pos))\n",
    "  ffn_output_pos = keras.layers.Dropout(dropout)(ffn_output_pos)\n",
    "   \n",
    "  ffn_output_slot = keras.layers.Dense(d_model)(keras.layers.Dense(dff, activation='relu')(out_1_slot))\n",
    "  ffn_output_slot = keras.layers.Dropout(dropout)(ffn_output_slot)\n",
    "\n",
    "  out_2_pos = LayerNormalization()(keras.layers.Add()([ffn_output_slot,out_1_pos]))\n",
    "  out_2_slot = LayerNormalization()(keras.layers.Add()([ffn_output_pos,out_1_slot]))\n",
    "\n",
    "  return out_2_slot,out_2_pos\n",
    "\n",
    "\n",
    "###################\n",
    "##### cross #####\n",
    "###################\n",
    "\n",
    "def new_trans_cross(input_slot_pos,d_model,num_heads,dff,dropout):\n",
    "  input_slot = input_slot_pos[0]\n",
    "  input_pos = input_slot_pos[1]\n",
    "  depth = d_model/num_heads\n",
    "\n",
    "  q_slot_1 = keras.layers.Dense(d_model)(input_slot)\n",
    "  k_slot_1 = keras.layers.Dense(d_model)(input_slot)\n",
    "  v_slot_1 = keras.layers.Dense(d_model)(input_slot)\n",
    "  q_pos_1 = keras.layers.Dense(d_model)(input_pos)\n",
    "  k_pos_1 = keras.layers.Dense(d_model)(input_pos)\n",
    "  v_pos_1 = keras.layers.Dense(d_model)(input_pos)\n",
    "\n",
    "  qw_slot_1 = split_heads(q_slot_1,num_heads)  \n",
    "  kw_slot_1 = split_heads(k_slot_1,num_heads)  \n",
    "  vw_slot_1 = split_heads(v_slot_1,num_heads) \n",
    "\n",
    "  qw_pos_1 = split_heads(q_pos_1,num_heads)  \n",
    "  kw_pos_1 = split_heads(k_pos_1,num_heads)  \n",
    "  vw_pos_1 = split_heads(v_pos_1,num_heads) \n",
    "\n",
    "  scaled_attention_pos = scaled_dot_product_attention(qw_pos_1, kw_slot_1, vw_slot_1, depth)\n",
    "  scaled_attention_pos = keras.layers.Permute((2,1,3))(scaled_attention_pos)  \n",
    "  concat_attention_pos = keras.layers.Reshape((-1, d_model))(scaled_attention_pos)\n",
    "  concat_attention_pos = keras.layers.Dense(d_model)(concat_attention_pos)\n",
    "  concat_attention_pos = keras.layers.Dropout(dropout)(concat_attention_pos)\n",
    "  out_1_pos = LayerNormalization()(keras.layers.Add()([q_pos_1,concat_attention_pos]))\n",
    "  ffn_output_pos = keras.layers.Dense(d_model)(keras.layers.Dense(dff, activation='relu')(out_1_pos))\n",
    "  ffn_output_pos = keras.layers.Dropout(dropout)(ffn_output_pos)\n",
    "  out_2_pos = LayerNormalization()(keras.layers.Add()([ffn_output_pos,out_1_pos]))\n",
    "\n",
    "\n",
    "  scaled_attention_slot = scaled_dot_product_attention(qw_slot_1, kw_pos_1, vw_pos_1, depth)\n",
    "  scaled_attention_slot = keras.layers.Permute((2,1,3))(scaled_attention_slot)\n",
    "  concat_attention_slot = keras.layers.Reshape((-1, d_model))(scaled_attention_slot)\n",
    "  concat_attention_slot = keras.layers.Dense(d_model)(concat_attention_slot)\n",
    "  concat_attention_slot = keras.layers.Dropout(dropout)(concat_attention_slot)\n",
    "  out_1_slot = LayerNormalization()(keras.layers.Add()([q_slot_1,concat_attention_slot]))\n",
    "  ffn_output_slot = keras.layers.Dense(d_model)(keras.layers.Dense(dff, activation='relu')(out_1_slot))\n",
    "  ffn_output_slot = keras.layers.Dropout(dropout)(ffn_output_slot)\n",
    "  out_2_slot = LayerNormalization()(keras.layers.Add()([ffn_output_slot,out_1_slot]))\n",
    "\n",
    "  return out_2_slot,out_2_pos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "INOWQuRbAoIo"
   },
   "outputs": [],
   "source": [
    "###@@@ POSITION ADDED @@@###\n",
    "def get_model_single_mode_position(mode, encoder_num=1):   \n",
    "  input_slot_initial=keras.layers.Input(shape=(50,100), name='Slot-Embed-initial')\n",
    "  input_pos_initial=keras.layers.Input(shape=(50,100), name='POS-Embed-initial')\n",
    "\n",
    "  input_slot = TrigPosEmbedding(\n",
    "        mode=TrigPosEmbedding.MODE_ADD,\n",
    "        name='Encoder-PosEmbedding-SLOT',\n",
    "   )(input_slot_initial)\n",
    "\n",
    "  input_pos = TrigPosEmbedding(\n",
    "        mode=TrigPosEmbedding.MODE_ADD,\n",
    "        name='Encoder-PosEmbedding-POS',\n",
    "   )(input_pos_initial)\n",
    "  \n",
    "  d_model = 768\n",
    "  num_heads = 3\n",
    "  dff = 1296\n",
    "  dropout = 0.5\n",
    "  if mode=='noex':\n",
    "    slot_out_2, pos_out_2 = new_trans_noex([input_slot,input_pos],d_model, num_heads, dff, dropout)\n",
    "    for i in range(encoder_num-1):\n",
    "      slot_out_2, pos_out_2 = new_trans_noex([slot_out_2,pos_out_2],d_model, num_heads, dff, dropout)\n",
    "  elif mode=='bf':\n",
    "    slot_out_2, pos_out_2 = new_trans_bf([input_slot,input_pos],d_model, num_heads, dff, dropout)\n",
    "    for i in range(encoder_num-1):\n",
    "      slot_out_2, pos_out_2 = new_trans_bf([slot_out_2,pos_out_2],d_model, num_heads, dff, dropout)\n",
    "  elif mode=='af':\n",
    "    slot_out_2, pos_out_2 = new_trans_af([input_slot,input_pos],d_model, num_heads, dff, dropout)\n",
    "    for i in range(encoder_num-1):\n",
    "      slot_out_2, pos_out_2 = new_trans_af([slot_out_2,pos_out_2],d_model, num_heads, dff, dropout)\n",
    "  else:\n",
    "    slot_out_2, pos_out_2 = new_trans_cross([input_slot,input_pos],d_model, num_heads, dff, dropout)\n",
    "    for i in range(encoder_num-1):\n",
    "      slot_out_2, pos_out_2 = new_trans_cross([slot_out_2,pos_out_2],d_model, num_heads, dff, dropout)\n",
    "  \n",
    "  \n",
    "  slot_out_2 = keras.layers.Dropout(0.1)(slot_out_2)\n",
    "  pos_out_2 = keras.layers.Dropout(0.1)(pos_out_2)\n",
    "\n",
    "  print(\"slot_out_2\", slot_out_2.shape)\n",
    "  print(\"pos_out_2\", pos_out_2.shape)\n",
    "  print(\"*****\")\n",
    "  # intent_out_logits=keras.layers.Flatten()(intent_out_2)\n",
    "  # intent_out_logits=keras.layers.Dense(pos_num, activation=\"softmax\", name=\"intent_output\")(intent_out_logits)\n",
    "  #########modify here by Jie(intent(pos) should has the same shape with slot_out_logits\n",
    "  \n",
    "  slot_out_logits= keras.layers.TimeDistributed(keras.layers.Dense(slot_num, activation=\"softmax\"), input_shape=(seq_length, slot_out_2.shape[-1]), name=\"slot_output\")(slot_out_2)\n",
    "  pos_out_logits= keras.layers.TimeDistributed(keras.layers.Dense(pos_num, activation=\"softmax\"), input_shape=(seq_length, pos_out_2.shape[-1]), name=\"pos_output\")(pos_out_2)\n",
    "  \n",
    "  return keras.models.Model(inputs=[input_slot_initial, input_pos_initial], outputs=[slot_out_logits, pos_out_logits])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "QWOzXFtNJLJf"
   },
   "outputs": [],
   "source": [
    "#####Jie: don't use this function#####\n",
    "###@@@ POSITION ADDED @@@###\n",
    "def get_model_single_mode_position_s2i(mode, encoder_num=1):   \n",
    "  input_slot_initial=keras.layers.Input(shape=(50,300), name='Slot-Embed-initial')\n",
    "  input_pos_initial=keras.layers.Input(shape=(50,300), name='POS-Embed-initial')\n",
    "  \n",
    "  input_slot = TrigPosEmbedding(\n",
    "        mode=TrigPosEmbedding.MODE_ADD,\n",
    "        name='Encoder-PosEmbedding-SLOT',\n",
    "   )(input_slot_initial)\n",
    "\n",
    "  input_pos = TrigPosEmbedding(\n",
    "        mode=TrigPosEmbedding.MODE_ADD,\n",
    "        name='Encoder-PosEmbedding-POS',\n",
    "   )(input_pos_initial)\n",
    "\n",
    "  d_model = 768\n",
    "  num_heads = 3\n",
    "  dff = 1296\n",
    "  dropout = 0.5\n",
    "  if mode=='noex':\n",
    "    slot_out_2, pos_out_2 = new_trans_noex([input_slot,input_pos],d_model, num_heads, dff, dropout)\n",
    "    for i in range(encoder_num-1):\n",
    "      slot_out_2, pos_out_2 = new_trans_noex([slot_out_2,pos_out_2],d_model, num_heads, dff, dropout)\n",
    "  elif mode=='bf':\n",
    "    slot_out_2, pos_out_2 = new_trans_bf([input_slot,input_pos],d_model, num_heads, dff, dropout)\n",
    "    for i in range(encoder_num-1):\n",
    "      slot_out_2, pos_out_2 = new_trans_bf([slot_out_2,pos_out_2],d_model, num_heads, dff, dropout)\n",
    "  elif mode=='af':\n",
    "    slot_out_2, pos_out_2 = new_trans_af([input_slot,input_pos],d_model, num_heads, dff, dropout)\n",
    "    for i in range(encoder_num-1):\n",
    "      slot_out_2, pos_out_2 = new_trans_af([slot_out_2,pos_out_2],d_model, num_heads, dff, dropout)\n",
    "  else:\n",
    "    slot_out_2, pos_out_2 = new_trans_cross([input_slot,input_pos],d_model, num_heads, dff, dropout)\n",
    "    for i in range(encoder_num-1):\n",
    "      slot_out_2, pos_out_2 = new_trans_cross([slot_out_2,pos_out_2],d_model, num_heads, dff, dropout)\n",
    "\n",
    "\n",
    "  slot_out_2 = keras.layers.Dropout(0.1)(slot_out_2)\n",
    "  pos_out_2 = keras.layers.Dropout(0.1)(pos_out_2)\n",
    "\n",
    "  # intent_out_flatten = keras.layers.Flatten()(intent_out_2)\n",
    "  \n",
    "  #slot gate\n",
    "  slot_gate_logits = keras.layers.TimeDistributed(keras.layers.Dense(slot_num, activation=\"softmax\"), input_shape=(seq_length, slot_out_2.shape[-1]), name=\"slot_output\")(slot_out_2)\n",
    "  #slot_out_logits=CRF(slot_num, sparse_target=True, name='slot_output')(slot_out_2)\n",
    "\n",
    "  #pos gate\n",
    "  pos_gate_logits = keras.layers.TimeDistributed(keras.layers.Dense(pos_num, activation=\"softmax\"), input_shape=(seq_length, pos_out_2.shape[-1]), name=\"pos_output\")(pos_out_2)\n",
    "  \n",
    "  # slot_out_flatten =  keras.layers.Flatten()(slot_gate_logits)\n",
    "  # intent_gate = keras.layers.Concatenate(axis=-1)([intent_out_flatten,slot_out_flatten])\n",
    "  # intent_gate_logits=keras.layers.Dense(pos_num, activation=\"softmax\", name=\"intent_output\")(intent_gate)\n",
    "\n",
    "  return keras.models.Model(inputs=[input_slot_initial, input_pos_initial], outputs=[slot_gate_logits, pos_gate_logits])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "bxN8sqNLeNM3"
   },
   "outputs": [],
   "source": [
    "###@@@ POSITION ADDED @@@###\n",
    "def get_model_single_mode_position_p2s(mode, encoder_num=1):   \n",
    "  input_slot_initial=keras.layers.Input(shape=(50,300), name='Slot-Embed-initial')\n",
    "  input_pos_initial=keras.layers.Input(shape=(50,300), name='Pos-Embed-initial')\n",
    "\n",
    "  input_slot = TrigPosEmbedding(\n",
    "        mode=TrigPosEmbedding.MODE_ADD,\n",
    "        name='Encoder-PosEmbedding-SLOT',\n",
    "   )(input_slot_initial)\n",
    "\n",
    "  input_pos = TrigPosEmbedding(\n",
    "        mode=TrigPosEmbedding.MODE_ADD,\n",
    "        name='Encoder-PosEmbedding-POS',\n",
    "   )(input_pos_initial)\n",
    "    \n",
    "  d_model = 768\n",
    "  num_heads = 3\n",
    "  dff = 1296\n",
    "  dropout = 0.5\n",
    "  if mode=='noex':\n",
    "    slot_out_2, pos_out_2 = new_trans_noex([input_slot,input_pos],d_model, num_heads, dff, dropout)\n",
    "    for i in range(encoder_num-1):\n",
    "      slot_out_2, pos_out_2 = new_trans_noex([slot_out_2,pos_out_2],d_model, num_heads, dff, dropout)\n",
    "  elif mode=='bf':\n",
    "    slot_out_2, pos_out_2 = new_trans_bf([input_slot,input_pos],d_model, num_heads, dff, dropout)\n",
    "    for i in range(encoder_num-1):\n",
    "      slot_out_2, pos_out_2 = new_trans_bf([slot_out_2,pos_out_2],d_model, num_heads, dff, dropout)\n",
    "  elif mode=='af':\n",
    "    slot_out_2, pos_out_2 = new_trans_af([input_slot,input_pos],d_model, num_heads, dff, dropout)\n",
    "    for i in range(encoder_num-1):\n",
    "      slot_out_2, pos_out_2 = new_trans_af([slot_out_2,pos_out_2],d_model, num_heads, dff, dropout)\n",
    "  else:\n",
    "    slot_out_2, pos_out_2 = new_trans_cross([input_slot,input_pos],d_model, num_heads, dff, dropout)\n",
    "    for i in range(encoder_num-1):\n",
    "      slot_out_2, pos_out_2 = new_trans_cross([slot_out_2,pos_out_2],d_model, num_heads, dff, dropout)\n",
    "\n",
    "  \n",
    "\n",
    "  print(\"slot_out_2\", slot_out_2.shape)\n",
    "  print(\"pos_out_2\", pos_out_2.shape)\n",
    "  print(\"*****\")\n",
    "  \n",
    "  slot_out_2 = keras.layers.Dropout(0.1)(slot_out_2)\n",
    "  pos_out_2 = keras.layers.Dropout(0.1)(pos_out_2)\n",
    "\n",
    "  # pos_out_flatten = keras.layers.Flatten()(pos_out_2)\n",
    "  # pos_out_logits=keras.layers.Dense(pos_num, activation=\"softmax\", name=\"pos_encoder_output\")(pos_out_flatten)\n",
    "\n",
    "  pos_out_logits= keras.layers.TimeDistributed(keras.layers.Dense(pos_num, activation=\"softmax\"), input_shape=(seq_length, pos_out_2.shape[-1]), name=\"pos_encoder_outpout\")(pos_out_2)\n",
    "  slot_out_logits= keras.layers.TimeDistributed(keras.layers.Dense(slot_num, activation=\"softmax\"), input_shape=(seq_length, slot_out_2.shape[-1]), name=\"slot_encoder_outpout\")(slot_out_2)\n",
    "  \n",
    "  #slot gate\n",
    "  # pos_out_repeat = keras.layers.RepeatVector(seq_length)(pos_out_logits)\n",
    "  #######Question here: pos_out_logits.shape[None, 50, 46], slot_out_2.shape[None, 50, 768]\n",
    "  #######slot_gate.shape[None, 50, 814]\n",
    "  slot_gate = keras.layers.Concatenate(axis=-1)([slot_out_2,pos_out_logits])\n",
    "  slot_gate_logits = keras.layers.TimeDistributed(keras.layers.Dense(slot_num, activation=\"softmax\"), input_shape=(seq_length, slot_gate.shape[-1]), name=\"slot_output\")(slot_gate)\n",
    "\n",
    "  \n",
    "  #pos gate\n",
    "  pos_gate_logits = keras.layers.TimeDistributed(keras.layers.Dense(pos_num, activation=\"softmax\"), input_shape=(seq_length, pos_out_2.shape[-1]), name=\"pos_output\")(pos_out_2)\n",
    "\n",
    "  return keras.models.Model(inputs=[input_slot_initial, input_pos_initial], outputs=[slot_gate_logits, pos_gate_logits])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "###@@@ POSITION ADDED @@@###\n",
    "def get_model_single_mode_position_bidir(mode, encoder_num=1):   \n",
    "  input_slot_initial=keras.layers.Input(shape=(50,100), name='Slot-Embed-initial')\n",
    "  input_pos_initial=keras.layers.Input(shape=(50,100), name='POS-Embed-initial')\n",
    "\n",
    "  input_slot = TrigPosEmbedding(\n",
    "        mode=TrigPosEmbedding.MODE_ADD,\n",
    "        name='Encoder-PosEmbedding-SLOT',\n",
    "   )(input_slot_initial)\n",
    "\n",
    "  input_pos = TrigPosEmbedding(\n",
    "        mode=TrigPosEmbedding.MODE_ADD,\n",
    "        name='Encoder-PosEmbedding-POS',\n",
    "   )(input_pos_initial)\n",
    "    \n",
    "  d_model=768\n",
    "  num_heads=3\n",
    "  dff = 1296\n",
    "  dropout=0.5\n",
    "  if mode=='noex':\n",
    "    slot_out_2, pos_out_2 = new_trans_noex([input_slot,input_pos], d_model, num_heads, dff, dropout)\n",
    "    for i in range(encoder_num-1):\n",
    "      slot_out_2, pos_out_2 = new_trans_noex([slot_out_2,pos_out_2],d_model, num_heads, dff, dropout)\n",
    "  elif mode=='bf':\n",
    "    slot_out_2, pos_out_2 = new_trans_bf([input_slot,input_pos],d_model, num_heads, dff, dropout)\n",
    "    for i in range(encoder_num-1):\n",
    "      slot_out_2, pos_out_2 = new_trans_bf([slot_out_2,pos_out_2],d_model, num_heads, dff, dropout)\n",
    "  elif mode=='af':\n",
    "    slot_out_2, pos_out_2 = new_trans_af([input_slot,input_pos],d_model, num_heads, dff, dropout)\n",
    "    for i in range(encoder_num-1):\n",
    "      slot_out_2, pos_out_2 = new_trans_af([slot_out_2,pos_out_2],d_model, num_heads, dff, dropout)\n",
    "  else:\n",
    "    slot_out_2, pos_out_2 = new_trans_cross([input_slot,input_pos],d_model, num_heads, dff, dropout)\n",
    "    for i in range(encoder_num-1):\n",
    "      slot_out_2, pos_out_2 = new_trans_cross([slot_out_2,pos_out_2],d_model, num_heads, dff, dropout)\n",
    "\n",
    "\n",
    "  slot_out_2 = keras.layers.Dropout(0.1)(slot_out_2)\n",
    "  pos_out_2 = keras.layers.Dropout(0.1)(pos_out_2)\n",
    "\n",
    "  # intent_out_flatten = keras.layers.Flatten()(intent_out_2)\n",
    "  # intent_out_logits=keras.layers.Dense(pos_num, activation=\"softmax\", name=\"intent_encoder_output\")(intent_out_flatten)\n",
    "\n",
    "  slot_out_logits= keras.layers.TimeDistributed(keras.layers.Dense(slot_num, activation=\"softmax\"), input_shape=(seq_length, slot_out_2.shape[-1]), name=\"slot_encoder_output\")(slot_out_2)\n",
    "  pos_out_logits= keras.layers.TimeDistributed(keras.layers.Dense(pos_num, activation=\"softmax\"), input_shape=(seq_length, pos_out_2.shape[-1]), name=\"pos_encoder_output\")(pos_out_2)\n",
    "  \n",
    "  # #slot gate\n",
    "  # pos_out_flatten =  keras.layers.Flatten()(pos_out_logits)  \n",
    "  # pos_gate = keras.layers.Concatenate(axis=-1)([intent_out_flatten,slot_out_flatten])\n",
    "    \n",
    "    \n",
    "  #slot gate\n",
    "  # pos_out_repeat = keras.layers.RepeatVector(seq_length)(pos_out_logits)\n",
    "  slot_gate = keras.layers.Concatenate(axis=-1)([slot_out_2,pos_out_logits])\n",
    "  slot_gate_logits = keras.layers.TimeDistributed(keras.layers.Dense(slot_num, activation=\"softmax\"), input_shape=(seq_length, slot_gate.shape[-1]), name=\"slot_output\")(slot_gate)\n",
    "\n",
    "\n",
    "  #intent gate\n",
    "  # slot_out_repeat = keras.layers.RepeatVector(seq_length)(slot_out_logits)\n",
    "  pos_gate = keras.layers.Concatenate(axis=-1)([pos_out_2,slot_out_logits])\n",
    "  pos_gate_logits = keras.layers.TimeDistributed(keras.layers.Dense(pos_num, activation=\"softmax\"), input_shape=(seq_length, pos_gate.shape[-1]), name=\"pos_output\")(pos_gate)\n",
    "\n",
    "\n",
    "  # slot_out_flatten =  keras.layers.Flatten()(slot_out_logits)\n",
    "  # intent_gate = keras.layers.Concatenate(axis=-1)([intent_out_flatten,slot_out_flatten])\n",
    "  # intent_gate_logits=keras.layers.Dense(pos_num, activation=\"softmax\", name=\"intent_output\")(intent_gate)\n",
    "\n",
    "  return keras.models.Model(inputs=[input_slot_initial, input_pos_initial], outputs=[slot_gate_logits, pos_gate_logits])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "EH04bQDI7oPC"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "def evaluate(trans_model, slot_X_train, slot_X_dev, slot_X_test, pos_X_train, pos_X_dev, pos_X_test, slot_y_train, slot_y_dev, slot_y_test, pos_y_train, pos_y_dev, pos_y_test, batch_size=8, epoch = 10, learningRate=0.001):\n",
    "\n",
    "  \n",
    "  learningRate=learningRate\n",
    "\n",
    "  losses = {\n",
    "\t\"slot_output\": \"sparse_categorical_crossentropy\",\n",
    "\t\"pos_output\": \"sparse_categorical_crossentropy\",\n",
    "  }\n",
    "\n",
    "  lossWeights = {\"slot_output\": 1.0, \"pos_output\": 1.0}\n",
    "\n",
    "  trans_model.compile(\n",
    "      optimizer= keras.optimizers.Adam(learning_rate=learningRate),\n",
    "      loss=losses,\n",
    "      loss_weights=lossWeights,\n",
    "      metrics = [\"accuracy\"]\n",
    "  )\n",
    "  \n",
    "\n",
    "  trans_model.summary()\n",
    "\n",
    "  current_time = datetime.now()\n",
    "  \n",
    "#   intent_y_train = np.array([t for t in intent_y_train])\n",
    "#   intent_y_dev = np.array([t for t in intent_y_dev])\n",
    "  \n",
    "  trans_model.fit([slot_X_train, pos_X_train], [slot_y_train, pos_y_train], validation_data=([slot_X_dev, pos_X_dev], [slot_y_dev, pos_y_dev]), batch_size=batch_size, epochs=epoch)\n",
    "  #trans_model.fit([slot_X_train, intent_X_train], [slot_y_train, intent_y_train], batch_size=batch_size, epochs=epoch)\n",
    "  train_time=datetime.now() - current_time\n",
    "  print(\"Training took time: \", train_time)\n",
    "\n",
    "\n",
    "  current_time = datetime.now()\n",
    "  scores = trans_model.evaluate([slot_X_test, pos_X_test], [slot_y_test, pos_y_test], batch_size = batch_size, verbose = 0)\n",
    "  eval_time=datetime.now() - current_time\n",
    "  print(\"Evaluation took time: \", eval_time)\n",
    "  eval_acc=(scores[-1]+scores[-2])/2\n",
    "  print(\"Evaluation accuracy: \",eval_acc)\n",
    "\n",
    "  #Predict classes\n",
    "  preds = trans_model.predict([slot_X_test, pos_X_test], batch_size = batch_size)\n",
    "\n",
    "  return eval_acc, train_time, eval_time, slot_y_test, pos_y_test, np.argmax(preds[0], axis=-1), np.argmax(preds[1], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fki3k2Jr9ruL"
   },
   "source": [
    "# Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q1JR8D179uxl",
    "outputId": "7429d9ac-cfd7-46fb-81e4-ecf8b8c5bf1c"
   },
   "outputs": [],
   "source": [
    "# !pip install seqeval\n",
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import precision_score\n",
    "from seqeval.metrics import recall_score\n",
    "from seqeval.metrics import classification_report as seqeval_classification_report\n",
    "from sklearn.metrics import f1_score as f_score\n",
    "from sklearn.metrics import recall_score as re_score\n",
    "from sklearn.metrics import precision_score as pre_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "znRTBkIG9xsN"
   },
   "outputs": [],
   "source": [
    "ind2slot={v:k for k,v in list(integer_mapping_slot.items())}\n",
    "ind2pos={v:k for k,v in list(integer_mapping_pos.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ldHzYWAa9yJ4"
   },
   "outputs": [],
   "source": [
    "#calculate sentence senmantic\n",
    "def sentence_senm(correct_poss, pos_outpus, correct_slots, slot_outputs):\n",
    "  # if(type(pred_poss[0]) is np.ndarray):\n",
    "  #   pred_poss=[x[0] for x in pred_poss]\n",
    "  # correct_poss=np.array(correct_poss)\n",
    "  # pred_poss=np.array(pred_poss)\n",
    "  # print(\"correct_poss \", correct_poss)\n",
    "  # print(\"pred_poss\", pred_poss)\n",
    "  # semantic_error = (pred_poss==correct_poss)\n",
    "  # print(semantic_error)\n",
    "  \n",
    "  slot_outputs_origin=[]\n",
    "  correct_slots_origin=[]\n",
    "  \n",
    "  for s_out, s_cor in zip (slot_outputs,correct_slots):\n",
    "    if type(s_out[0])==list:\n",
    "      new_seq_true= [x for x in s_cor]\n",
    "    else:\n",
    "      new_seq_true=s_cor\n",
    "\n",
    "    new_seq_true=np.trim_zeros(new_seq_true)\n",
    "    correct_slots_origin.append(new_seq_true)\n",
    "    \n",
    "    new_seq_temp= s_out[:len(new_seq_true)]\n",
    "    slot_outputs_origin.append(new_seq_temp)\n",
    "    \n",
    "  index=0\n",
    "\n",
    "  for t,p in zip(correct_slots_origin, slot_outputs_origin):\n",
    "    assert len(t)==len(p)\n",
    "    \n",
    "    for j in range(len(t)):\n",
    "      if p[j] != t[j]:\n",
    "        semantic_error[index]=False\n",
    "        \n",
    "        break\n",
    "    index+=1\n",
    "  # print(\"semantic_error: \", semantic_error.shape)\n",
    "  \n",
    "  semantic_error=semantic_error.astype(float)\n",
    "  semantic_error=np.mean(semantic_error)*100.0\n",
    "  \n",
    "  \n",
    "  return semantic_error\n",
    "\n",
    "\n",
    "\n",
    "def print_semantic(correct_poss, pred_poss, correct_slots, slot_outputs, test_seq):\n",
    "  # if(type(pred_poss[0]) is np.ndarray):\n",
    "  #   pred_poss=[x[0] for x in pred_poss]\n",
    "\n",
    "  count=1\n",
    "  for s_out, s_cor, p_out, p_cor, seq in zip (slot_outputs,correct_slots, pred_poss, correct_poss, test_seq):\n",
    "    \n",
    "    #print()\n",
    "    if type(s_out[0])==list:\n",
    "      new_seq_true= [x for x in s_cor]\n",
    "    else:\n",
    "      new_seq_true=s_cor\n",
    "    \n",
    "    if type(p_out[0])==list:\n",
    "      new_pos_true= [x for x in p_cor]\n",
    "    else:\n",
    "      new_pos_true=p_cor\n",
    "    \n",
    "    \n",
    "    new_seq_true=np.trim_zeros(new_seq_true)\n",
    "    #correct_slots_origin.append(new_seq_true)\n",
    "    new_seq_temp= s_out[:len(new_seq_true)]\n",
    "    new_seq_true=[ind2slot[x] for x in new_seq_true]\n",
    "    new_seq_temp=[ind2slot[x] for x in new_seq_temp]\n",
    "    \n",
    "    new_pos_true=np.trim_zeros(new_pos_true)\n",
    "    new_pos_temp= p_out[:len(new_pos_true)]\n",
    "    new_pos_true=[ind2pos[int(x)] for x in new_pos_true]\n",
    "    new_pos_temp=[ind2pos[int(x)] for x in new_pos_temp]\n",
    "    print(\"#\"*120)\n",
    "    print(seq)\n",
    "    print(\"slot true: \", new_seq_true)\n",
    "    print(\"slot predict: \", new_seq_temp)\n",
    "    print(\"pos true: \", new_pos_true)\n",
    "    print(\"pos predict\", new_pos_temp)\n",
    "    print(\"#\"*120)\n",
    "\n",
    "def pos_f1_each_class(y_true, y_pred):\n",
    "  \n",
    "  return f_score(y_true, y_pred, labels=list(np.unique(np.array(y_true))), average=None)\n",
    "\n",
    "#calculate the precision, recall, f1 for pos classification\n",
    "def pos_accuracy(y_true, y_pred):\n",
    "  new_y_true=[]\n",
    "  new_y_pred=[]\n",
    "  \n",
    "  \n",
    "  for se, pre in zip(y_true, y_pred):\n",
    "    \n",
    "    new_seq_true=np.trim_zeros(se)\n",
    "    #print(new_seq_true)\n",
    "    new_y_true.extend(list(new_seq_true))\n",
    "    #print(new_seq_true)\n",
    "    \n",
    "    \n",
    "    if (type(pre[0])==list):\n",
    "      pre=[x[0] for x in pre]\n",
    "    new_seq_temp= list(pre)[:len(new_seq_true)]\n",
    "    #print(new_seq_temp)\n",
    "    #print()\n",
    "    new_y_pred.extend(new_seq_temp)\n",
    "  #print(new_y_true)\n",
    "  new_y_true = [int(t) for t in new_y_true]\n",
    "  # print(\"pos true type: \", type(new_y_true[5]))\n",
    "  # print(\"pos true length: \", new_y_true[5])\n",
    "  label=list(set(new_y_true))\n",
    "\n",
    "  correct=0\n",
    "  for i,j in zip(new_y_true, new_y_pred):\n",
    "    if (i==j):\n",
    "      correct+=1\n",
    "  accuracy=correct/len(new_y_true)\n",
    "  \n",
    "  return accuracy,pre_score(new_y_true, new_y_pred, labels=label, average='micro'), re_score(new_y_true, new_y_pred, labels=label, average='micro'), f_score(new_y_true, new_y_pred, labels=label, average='micro')\n",
    "    \n",
    "\n",
    "#calculate the precision, recall, f1 for slot filling\n",
    "def slot_accuracy(y_true, y_pred):\n",
    "  new_y_true=[]\n",
    "  new_y_pred=[]\n",
    "  \n",
    "  \n",
    "  for se, pre in zip(y_true, y_pred):\n",
    "    \n",
    "    new_seq_true=np.trim_zeros(se)\n",
    "    #print(new_seq_true)\n",
    "    new_y_true.extend(list(new_seq_true))\n",
    "    #print(new_seq_true)\n",
    "    \n",
    "    \n",
    "    if (type(pre[0])==list):\n",
    "      pre=[x[0] for x in pre]\n",
    "    new_seq_temp= list(pre)[:len(new_seq_true)]\n",
    "    #print(new_seq_temp)\n",
    "    #print()\n",
    "    new_y_pred.extend(new_seq_temp)\n",
    "  #print(new_y_true)\n",
    "  # new_y_true = [int(t) for t in new_y_true]\n",
    "  # print(\"slot true type: \", type(new_y_true[5]))\n",
    "  # print(\"slot true length: \", new_y_true[5])\n",
    "  label=list(set(new_y_true))\n",
    "\n",
    "  correct=0\n",
    "  for i,j in zip(new_y_true, new_y_pred):\n",
    "    if (i==j):\n",
    "      correct+=1\n",
    "  accuracy=correct/len(new_y_true)\n",
    "  \n",
    "  return accuracy,pre_score(new_y_true, new_y_pred, labels=label, average='micro'), re_score(new_y_true, new_y_pred, labels=label, average='micro'), f_score(new_y_true, new_y_pred, labels=label, average='micro')\n",
    "    \n",
    "  \n",
    "\n",
    "  \n",
    "def slot_accuracy_slot_based(y_true, y_pred):\n",
    "  new_y_true=[]\n",
    "  new_y_pred=[]\n",
    "  for se, pre in zip(y_true, y_pred):\n",
    "    #new_seq_true= [x for x in se]\n",
    "    new_seq_true= se\n",
    "    new_seq_true=np.trim_zeros(new_seq_true)\n",
    "    new_y_true.append([ind2slot[x] for x in new_seq_true])\n",
    "    new_seq_temp= pre[:len(new_seq_true)]\n",
    "    new_y_pred.append([ind2slot[x] for x in new_seq_temp])\n",
    "\n",
    "\n",
    "  y_accuracy = accuracy_score(new_y_true,new_y_pred)\n",
    "  y_f1 = f1_score(new_y_true,new_y_pred)\n",
    "  y_precison = precision_score(new_y_true,new_y_pred)\n",
    "  y_recall = recall_score(new_y_true,new_y_pred)\n",
    "  print()\n",
    "  print(seqeval_classification_report(new_y_true,new_y_pred, digits = 4))\n",
    "  print()\n",
    "  \n",
    "  return y_accuracy,y_precison,y_recall,y_f1       \n",
    "  \n",
    "\n",
    "def pos_accuracy_pos_based(y_true, y_pred):\n",
    "  new_y_true=[]\n",
    "  new_y_pred=[]\n",
    "  for se, pre in zip(y_true, y_pred):\n",
    "    #new_seq_true= [x for x in se]\n",
    "    new_seq_true= se\n",
    "    new_seq_true=np.trim_zeros(new_seq_true)\n",
    "    new_y_true.append([ind2slot[x] for x in new_seq_true])\n",
    "    new_seq_temp= pre[:len(new_seq_true)]\n",
    "    new_y_pred.append([ind2slot[x] for x in new_seq_temp])\n",
    "\n",
    "\n",
    "  y_accuracy = accuracy_score(new_y_true,new_y_pred)\n",
    "  y_f1 = f1_score(new_y_true,new_y_pred)\n",
    "  y_precison = precision_score(new_y_true,new_y_pred)\n",
    "  y_recall = recall_score(new_y_true,new_y_pred)\n",
    "  #print()\n",
    "  #print(seqeval_classification_report(new_y_true,new_y_pred))\n",
    "  #print()\n",
    "  \n",
    "  return y_accuracy,y_precison,y_recall,y_f1       \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I-JYMN_2-AiV",
    "outputId": "69b3e0c7-110f-42e7-fd2a-fd3a617e54cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 17:25:23.695492: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-01 17:25:25.413527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 17513 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:02:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slot_out_2 (None, 50, 768)\n",
      "pos_out_2 (None, 50, 768)\n",
      "*****\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Slot-Embed-initial (InputLayer  [(None, 50, 300)]   0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " Encoder-PosEmbedding-SLOT (Tri  (None, 50, 300)     0           ['Slot-Embed-initial[0][0]']     \n",
      " gPosEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " Pos-Embed-initial (InputLayer)  [(None, 50, 300)]   0           []                               \n",
      "                                                                                                  \n",
      " Encoder-PosEmbedding-POS (Trig  (None, 50, 300)     0           ['Pos-Embed-initial[0][0]']      \n",
      " PosEmbedding)                                                                                    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 50, 768)      231168      ['Encoder-PosEmbedding-SLOT[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 50, 768)      231168      ['Encoder-PosEmbedding-POS[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 50, 768)      231168      ['Encoder-PosEmbedding-POS[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 50, 3, 256)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 50, 768)      231168      ['Encoder-PosEmbedding-SLOT[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 50, 3, 256)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 50, 3, 256)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " permute_1 (Permute)            (None, 3, 50, 256)   0           ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 50, 3, 256)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " permute_4 (Permute)            (None, 3, 50, 256)   0           ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " permute_3 (Permute)            (None, 3, 50, 256)   0           ['reshape_3[0][0]']              \n",
      "                                                                                                  \n",
      " permute_6 (Permute)            (None, 3, 256, 50)   0           ['permute_1[0][0]']              \n",
      "                                                                                                  \n",
      " permute (Permute)              (None, 3, 50, 256)   0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " permute_8 (Permute)            (None, 3, 256, 50)   0           ['permute_4[0][0]']              \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 3, 50, 50)    0           ['permute_3[0][0]',              \n",
      "                                                                  'permute_6[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 50, 768)      231168      ['Encoder-PosEmbedding-SLOT[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)              (None, 3, 50, 50)    0           ['permute[0][0]',                \n",
      "                                                                  'permute_8[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 50, 768)      231168      ['Encoder-PosEmbedding-POS[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 3, 50, 50)    0           ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 50, 3, 256)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_4 (Lambda)              (None, 3, 50, 50)    0           ['lambda_3[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, 50, 3, 256)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 3, 50, 50)    0           ['lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " permute_2 (Permute)            (None, 3, 50, 256)   0           ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 3, 50, 50)    0           ['lambda_4[0][0]']               \n",
      "                                                                                                  \n",
      " permute_5 (Permute)            (None, 3, 50, 256)   0           ['reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)              (None, 3, 50, 256)   0           ['activation[0][0]',             \n",
      "                                                                  'permute_2[0][0]']              \n",
      "                                                                                                  \n",
      " lambda_5 (Lambda)              (None, 3, 50, 256)   0           ['activation_1[0][0]',           \n",
      "                                                                  'permute_5[0][0]']              \n",
      "                                                                                                  \n",
      " permute_7 (Permute)            (None, 50, 3, 256)   0           ['lambda_2[0][0]']               \n",
      "                                                                                                  \n",
      " permute_9 (Permute)            (None, 50, 3, 256)   0           ['lambda_5[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_6 (Reshape)            (None, 50, 768)      0           ['permute_7[0][0]']              \n",
      "                                                                                                  \n",
      " reshape_7 (Reshape)            (None, 50, 768)      0           ['permute_9[0][0]']              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 50, 768)      590592      ['reshape_6[0][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 50, 768)      590592      ['reshape_7[0][0]']              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 50, 768)      0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 50, 768)      0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 50, 768)      0           ['dense_3[0][0]',                \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 50, 768)      0           ['dense[0][0]',                  \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 50, 768)     1536        ['add[0][0]']                    \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 50, 768)     1536        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 50, 1296)     996624      ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 50, 1296)     996624      ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 50, 768)      996096      ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 50, 768)      996096      ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 50, 768)      0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 50, 768)      0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 50, 768)      0           ['dropout_1[0][0]',              \n",
      "                                                                  'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 50, 768)      0           ['dropout_3[0][0]',              \n",
      "                                                                  'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 50, 768)     1536        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 50, 768)     1536        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 50, 768)      0           ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 50, 768)      0           ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " pos_encoder_outpout (TimeDistr  (None, 50, 46)      35374       ['dropout_5[0][0]']              \n",
      " ibuted)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 50, 814)      0           ['dropout_4[0][0]',              \n",
      "                                                                  'pos_encoder_outpout[0][0]']    \n",
      "                                                                                                  \n",
      " slot_output (TimeDistributed)  (None, 50, 21)       17115       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " pos_output (TimeDistributed)   (None, 50, 46)       35374       ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,647,639\n",
      "Trainable params: 6,647,639\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 17:25:26.345575: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2489760000 exceeds 10% of free system memory.\n",
      "2023-02-01 17:25:28.174855: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2489760000 exceeds 10% of free system memory.\n",
      "2023-02-01 17:25:30.023582: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2489760000 exceeds 10% of free system memory.\n",
      "2023-02-01 17:25:31.305091: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2489760000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   16/41496 [..............................] - ETA: 8:22 - loss: 2.6351 - slot_output_loss: 0.5852 - pos_output_loss: 2.0498 - slot_output_accuracy: 0.8650 - pos_output_accuracy: 0.6837      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 17:25:36.698922: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41496/41496 [==============================] - 563s 13ms/step - loss: 0.4883 - slot_output_loss: 0.0313 - pos_output_loss: 0.4570 - slot_output_accuracy: 0.9934 - pos_output_accuracy: 0.8614 - val_loss: 0.4287 - val_slot_output_loss: 0.0727 - val_pos_output_loss: 0.3560 - val_slot_output_accuracy: 0.9906 - val_pos_output_accuracy: 0.8958\n",
      "Training took time:  0:09:29.342200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 17:34:55.686459: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1836840000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation took time:  0:03:02.951162\n",
      "Evaluation accuracy:  0.9413565695285797\n"
     ]
    }
   ],
   "source": [
    "#with bi-directional\n",
    "\n",
    "epoch=1\n",
    "batch_size = 1\n",
    "learningRate=4e-4\n",
    "\n",
    "# can choose: bf, af, cross\n",
    "# trans_model=get_model_single_mode_position('bf',1)\n",
    "\n",
    "# can choose: noex, bf, af, cross\n",
    "trans_model=get_model_single_mode_position_p2s('cross',1)\n",
    "\n",
    "# can choose: noex, bf, af, cross\n",
    "# trans_model=get_model_single_mode_position_bidir('bf',1)\n",
    "\n",
    "eval_acc, train_time, eval_time, test_Y_slot, test_Y_pos, preds_slot, preds_pos= evaluate(trans_model, train_seq_glove, dev_seq_glove, test_seq_glove, train_seq_glove, dev_seq_glove, test_seq_glove, np.expand_dims(train_slot_padded, -1), np.expand_dims(dev_slot_padded, -1), np.expand_dims(test_slot_padded, -1), np.expand_dims(train_pos_padded, -1), np.expand_dims(dev_pos_padded, -1), np.expand_dims(test_pos_padded, -1), batch_size, epoch, learningRate)\n",
    "\n",
    "\n",
    "test_Y_slot=np.squeeze(test_Y_slot, axis=-1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
