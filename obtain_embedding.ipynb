{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AAvodBPXYyw"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "n5z0rzHmYq3W"
   },
   "outputs": [],
   "source": [
    "# !pip install -U -q PyDrive\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "# # Authenticate and create the PyDrive client.\n",
    "# auth.authenticate_user()\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2lXN8F-ZPLGS",
    "outputId": "20901585-a571-4686-e79a-a6c1922c71c1"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "# import os\n",
    "# os.chdir('drive/My Drive/Colab Notebooks/Manchester Collaborate/[Joint-NLP][n2c2data]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.bfsu.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: pytorch-pretrained-bert in /mistgpu/site-packages (0.6.2)\n",
      "Requirement already satisfied: pytorch-nlp in /mistgpu/site-packages (0.5.0)\n",
      "Requirement already satisfied: torch>=0.4.1 in /mistgpu/site-packages (from pytorch-pretrained-bert) (1.12.1+cu113)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from pytorch-pretrained-bert) (4.64.0)\n",
      "Requirement already satisfied: regex in /mistgpu/site-packages (from pytorch-pretrained-bert) (2022.10.31)\n",
      "Requirement already satisfied: requests in /mistgpu/site-packages (from pytorch-pretrained-bert) (2.28.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pytorch-pretrained-bert) (1.22.3)\n",
      "Requirement already satisfied: boto3 in /mistgpu/site-packages (from pytorch-pretrained-bert) (1.26.15)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.2.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.15 in /mistgpu/site-packages (from boto3->pytorch-pretrained-bert) (1.29.15)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /mistgpu/site-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /mistgpu/site-packages (from boto3->pytorch-pretrained-bert) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch-pretrained-bert) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch-pretrained-bert) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mistgpu/site-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /mistgpu/site-packages (from requests->pytorch-pretrained-bert) (1.26.13)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.15->boto3->pytorch-pretrained-bert) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /mistgpu/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.15->boto3->pytorch-pretrained-bert) (1.12.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3090'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install\n",
    "!pip install pytorch-pretrained-bert pytorch-nlp\n",
    "\n",
    "# BERT imports\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertModel\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#% matplotlib inline\n",
    "\n",
    "# specify GPU device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "-0YPS20QXdEo"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "train_data = json.load(open(\"Data/train_ins_gs.json\"))\n",
    "dev_data = json.load(open(\"Data/valid_ins_gs.json\"))\n",
    "test_data = json.load(open(\"Data/test_ins_gs.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "Jwhbx5HBcm9K"
   },
   "outputs": [],
   "source": [
    "train_seq = train_data['sentence']\n",
    "dev_seq = dev_data['sentence']\n",
    "test_seq = test_data['sentence']\n",
    "train_slot = train_data['gt']\n",
    "dev_slot = dev_data['gt']\n",
    "test_slot = test_data['gt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20y8gap-RUrg"
   },
   "source": [
    "#POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z0G5vwRhRbW9",
    "outputId": "26f11b1c-7466-4677-997d-5a6c8fdfe15d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mist/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#obtain POS tag\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "n3e8mUD2RoZ8"
   },
   "outputs": [],
   "source": [
    "def tag_pos(train_sentences, dev_sentences, test_sentences):\n",
    "      pos_tagged_sent = []\n",
    "\n",
    "      pos_tagged_sent_train = []\n",
    "      for sent in train_sentences:\n",
    "          pos_tagged_sent.extend(pos_tag(sent))\n",
    "          pos_tagged_sent_train.append(pos_tag(sent))\n",
    "\n",
    "      pos_tagged_sent_dev = []\n",
    "      for sent in dev_sentences:\n",
    "          pos_tagged_sent_dev.append(pos_tag(sent))\n",
    "\n",
    "      pos_tagged_sent_test = []\n",
    "      for sent in test_sentences:\n",
    "          pos_tagged_sent_test.append(pos_tag(sent))\n",
    "\n",
    "      tags = list(set([i[1] for i in pos_tagged_sent]))\n",
    "\n",
    "      pos_input_dim = len(tags)\n",
    "      char_to_int = dict((c, i) for i, c in enumerate(tags))\n",
    "      int_to_char = dict((i, c) for i, c in enumerate(tags))\n",
    "\n",
    "      train_pos_encoded =[]\n",
    "      for i in range(len(pos_tagged_sent_train)):\n",
    "          temp = [pos[1] for pos in pos_tagged_sent_train[i]]\n",
    "          train_pos_encoded.append(temp)\n",
    "      \n",
    "      dev_pos_encoded =[]\n",
    "      for i in range(len(pos_tagged_sent_dev)):\n",
    "          temp = [pos[1] for pos in pos_tagged_sent_dev[i]]\n",
    "          dev_pos_encoded.append(temp)\n",
    "\n",
    "      test_pos_encoded =[]\n",
    "      for i in range(len(pos_tagged_sent_test)):\n",
    "          temp = [pos[1] for pos in pos_tagged_sent_test[i]]\n",
    "          test_pos_encoded.append(temp)\n",
    "\n",
    "      return train_pos_encoded, dev_pos_encoded, test_pos_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "tk7eB9LhSChv"
   },
   "outputs": [],
   "source": [
    "train_seq_pos, dev_seq_pos, test_seq_pos = tag_pos(train_seq, dev_seq, test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_intent = ['no intent' for _ in range(0, len(train_seq))]\n",
    "train_intent = tuple(train_intent)\n",
    "dev_intent = ['no intent' for _ in range(0, len(dev_seq))]\n",
    "dev_intent = tuple(dev_intent)\n",
    "test_intent = ['no intent' for _ in range(0, len(test_seq))]\n",
    "test_intent = tuple(test_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMnaWSrLSLVG",
    "outputId": "c958718b-34fa-4b53-a057-b515a42b442c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "Training data: 41496\n",
      "Development data: 4538\n",
      "Testing data: 30614\n",
      "####################################################################################################\n",
      "Data example:\n",
      "Sequence: \n",
      "['She', 'had', 'fevers', ',', 'weakness', ',', 'and', 'diarrhea', '.']\n",
      "\n",
      "Slots: \n",
      "['O', 'O', 'B-ADE', 'O', 'B-ADE', 'O', 'O', 'B-ADE', 'O']\n",
      "\n",
      "POS: \n",
      "['PRP', 'VBD', 'NNS', ',', 'NN', ',', 'CC', 'NN', '.']\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "print(\"#\"*100)\n",
    "print(\"Training data: \"+str(len(train_data['sentence'])))\n",
    "print(\"Development data: \"+str(len(dev_data['sentence'])))\n",
    "print(\"Testing data: \"+str(len(test_data['sentence'])))\n",
    "print(\"#\"*100)\n",
    "print(\"Data example:\")\n",
    "print(\"Sequence: \")\n",
    "print(test_seq[5])\n",
    "print()\n",
    "print(\"Slots: \")\n",
    "print(test_slot[5])\n",
    "print()\n",
    "print(\"POS: \")\n",
    "print(test_seq_pos[5])\n",
    "print(\"#\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ojJAX305SPy9",
    "outputId": "bd17cc86-ab89-4498-b8cf-e9bc4fca1d66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################################################\n",
      "Total slots: {'<UNK>', 'O', 'B-Strength', '<PAD>', 'B-Frequency', 'I-Drug', 'I-ADE', 'I-Strength', 'B-ADE', 'I-Dosage', 'I-Frequency', 'I-Duration', 'B-Drug', 'I-Reason', 'I-Form', 'B-Route', 'B-Dosage', '[CLS]', 'B-Form', 'B-Reason', 'B-Duration', 'I-Route', '[SEP]'}\n",
      "Total pos: {'JJ', '#', '<UNK>', '.', '<PAD>', ')', 'DT', 'RP', 'RBR', 'PDT', \"''\", 'VBP', '(', 'SYM', 'NN', 'TO', 'RB', 'PRP', 'VBG', 'EX', 'NNS', 'WP', 'FW', 'JJS', 'IN', 'NNPS', 'PRP$', 'VBD', ':', 'MD', 'WP$', 'VBZ', 'LS', 'RBS', '[CLS]', 'WDT', 'WRB', 'CD', 'CC', 'POS', 'NNP', ',', '$', 'UH', 'VB', 'JJR', 'VBN', '[SEP]'}\n",
      "#####################################################################################\n",
      "#####################################################################################\n",
      "Total slots type: 23\n",
      "Total pos type: 48\n",
      "Total intents type: 2\n",
      "#####################################################################################\n",
      "['B-Form', 'B-Reason', 'B-Duration', 'I-Route', '[SEP]']\n",
      "['UH', 'VB', 'JJR', 'VBN', '[SEP]']\n",
      "#####################################################################################\n"
     ]
    }
   ],
   "source": [
    "slot_all=set()\n",
    "pos_all=set()\n",
    "intent_all = set()\n",
    "\n",
    "for train_sequ, train_int in zip(train_slot, train_seq_pos):\n",
    "  for i in train_sequ:\n",
    "    slot_all.add(i)\n",
    "  for i in train_int:\n",
    "    pos_all.add(i)\n",
    "    \n",
    "for train_int in train_intent:\n",
    "  intent_all.add(train_int)\n",
    "\n",
    "#temp=['<PAD>']\n",
    "#temp.extend(list(slot_all))\n",
    "slot_all.add('<PAD>')\n",
    "slot_all.add('<UNK>')\n",
    "slot_all.add('[CLS]')\n",
    "slot_all.add('[SEP]')\n",
    "# slot_all.add('[CLS]')\n",
    "pos_all.add('<PAD>')\n",
    "pos_all.add('<UNK>')\n",
    "pos_all.add('[CLS]')\n",
    "pos_all.add('[SEP]')\n",
    "# pos_all=list(pos_all)\n",
    "intent_all.add('<UNK>')\n",
    "intent_all=list(intent_all)\n",
    "\n",
    "\n",
    "slot_num=len(slot_all)\n",
    "pos_num=len(pos_all)\n",
    "intent_num=len(intent_all)\n",
    "\n",
    "print(\"#\"*85)\n",
    "print(\"Total slots: \" + str(slot_all))\n",
    "print(\"Total pos: \" + str(pos_all))\n",
    "\n",
    "\n",
    "print(\"#\"*85)\n",
    "\n",
    "print(\"#\"*85)\n",
    "print(\"Total slots type: \" + str(slot_num))\n",
    "print(\"Total pos type: \" + str(pos_num))\n",
    "print(\"Total intents type: \" + str(intent_num))\n",
    "\n",
    "print(\"#\"*85)\n",
    "\n",
    "print(list(slot_all)[-5:])\n",
    "print(list(pos_all)[-5:])\n",
    "\n",
    "\n",
    "print(\"#\"*85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ta_Tifv_SWvc",
    "outputId": "c8a92314-4ced-448b-e577-5b59624e3bbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<UNK>': 1, 'B-ADE': 2, 'B-Dosage': 3, 'B-Drug': 4, 'B-Duration': 5, 'B-Form': 6, 'B-Frequency': 7, 'B-Reason': 8, 'B-Route': 9, 'B-Strength': 10, 'I-ADE': 11, 'I-Dosage': 12, 'I-Drug': 13, 'I-Duration': 14, 'I-Form': 15, 'I-Frequency': 16, 'I-Reason': 17, 'I-Route': 18, 'I-Strength': 19, 'O': 20, '[CLS]': 21, '[SEP]': 22}\n",
      "{'#': 0, '$': 1, \"''\": 2, '(': 3, ')': 4, ',': 5, '.': 6, ':': 7, '<PAD>': 8, '<UNK>': 9, 'CC': 10, 'CD': 11, 'DT': 12, 'EX': 13, 'FW': 14, 'IN': 15, 'JJ': 16, 'JJR': 17, 'JJS': 18, 'LS': 19, 'MD': 20, 'NN': 21, 'NNP': 22, 'NNPS': 23, 'NNS': 24, 'PDT': 25, 'POS': 26, 'PRP': 27, 'PRP$': 28, 'RB': 29, 'RBR': 30, 'RBS': 31, 'RP': 32, 'SYM': 33, 'TO': 34, 'UH': 35, 'VB': 36, 'VBD': 37, 'VBG': 38, 'VBN': 39, 'VBP': 40, 'VBZ': 41, 'WDT': 42, 'WP': 43, 'WP$': 44, 'WRB': 45, '[CLS]': 46, '[SEP]': 47}\n",
      "{'<UNK>': 0, 'no intent': 1}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lEnc_slot = LabelEncoder()\n",
    "lEnc_pos = LabelEncoder()\n",
    "lEnc_intent = LabelEncoder()\n",
    "\n",
    "slots=np.array(list(slot_all))\n",
    "pos=np.array(list(pos_all))\n",
    "intents=np.array(list(intent_all))\n",
    "lEnc_slot.fit(slots)\n",
    "lEnc_pos.fit(pos)\n",
    "lEnc_intent.fit(intents)\n",
    "\n",
    "integer_mapping_slot = {l: i for i, l in enumerate(lEnc_slot.classes_)}\n",
    "integer_mapping_pos = {l: i for i, l in enumerate(lEnc_pos.classes_)}\n",
    "integer_mapping_intent = {l: i for i, l in enumerate(lEnc_intent.classes_)}\n",
    "print(integer_mapping_slot)\n",
    "print(integer_mapping_pos)\n",
    "print(integer_mapping_intent)\n",
    "\n",
    "train_slot_transformed=[]\n",
    "dev_slot_transformed=[]\n",
    "test_slot_transformed=[]\n",
    "for seq_tr in train_slot:\n",
    "  label=[integer_mapping_slot[i] if i in integer_mapping_slot else integer_mapping_slot['<UNK>'] for i in seq_tr]\n",
    "  train_slot_transformed.append(label)\n",
    "\n",
    "for seq_de in dev_slot:\n",
    "  label=[integer_mapping_slot[i] if i in integer_mapping_slot else integer_mapping_slot['<UNK>'] for i in seq_de]\n",
    "  dev_slot_transformed.append(label)\n",
    "\n",
    "for seq_te in test_slot:\n",
    "  label=[integer_mapping_slot[i] if i in integer_mapping_slot else integer_mapping_slot['<UNK>'] for i in seq_te]\n",
    "  test_slot_transformed.append(label) \n",
    "    \n",
    "\n",
    "train_pos_transformed=[]\n",
    "dev_pos_transformed=[]\n",
    "test_pos_transformed=[]\n",
    "for seq_tr in train_seq_pos:\n",
    "  label=[integer_mapping_pos[i] if i in integer_mapping_pos else integer_mapping_pos['<UNK>'] for i in seq_tr]\n",
    "  train_pos_transformed.append(label)\n",
    "\n",
    "for seq_de in dev_seq_pos:\n",
    "  label=[integer_mapping_pos[i] if i in integer_mapping_pos else integer_mapping_pos['<UNK>'] for i in seq_de]\n",
    "  dev_pos_transformed.append(label)\n",
    "\n",
    "for seq_te in test_seq_pos:\n",
    "  label=[integer_mapping_pos[i] if i in integer_mapping_pos else integer_mapping_pos['<UNK>'] for i in seq_te]\n",
    "  test_pos_transformed.append(label) \n",
    "\n",
    "train_intent_transformed=lEnc_intent.transform(train_intent)\n",
    "dev_intent_transformed=[integer_mapping_intent[i] if i in integer_mapping_intent else integer_mapping_intent['<UNK>'] for i in dev_intent]\n",
    "test_intent_transformed=[integer_mapping_intent[i] if i in integer_mapping_intent else integer_mapping_intent['<UNK>'] for i in test_intent]\n",
    " \n",
    "# train_pos_transformed=lEnc_intent.transform(train_seq_pos)\n",
    "# dev_pos_transformed=[integer_mapping_intent[i] if i in integer_mapping_intent else integer_mapping_intent['<UNK>'] for i in dev_seq_pos]\n",
    "# test_pos_transformed=[integer_mapping_intent[i] if i in integer_mapping_intent else integer_mapping_intent['<UNK>'] for i in test_seq_pos]\n",
    " \n",
    "pad_slot_id=integer_mapping_slot['<PAD>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'admission', 'date', ':', '[', '*', '*', '217', '-', '7', '-', '5', '*', '*', ']', 'discharge', 'date', ':', '[', '*', '*', '217', '-', '7', '-', '7', '*', '*', ']', 'service', ':', 'surgery', 'allergies', ':', 'amoxicillin', '/', 'penicillin', '/', 'coum', '/', 'oxy', '/', 'meg', 'acetate', '/', 'rem', '/', 'rit', 'attending', 'name', '(', '[SEP]']\n",
      "['[CLS]', '[SEP]']\n",
      "['[CLS]', 'admission', 'date', ':', '[', '*', '*', '210', '-', '7', '-', '21', '*', '*', ']', 'discharge', 'date', ':', '[SEP]']\n",
      "[21, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 4, 20, 4, 20, 4, 20, 4, 20, 4, 4, 20, 4, 20, 4, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 22]\n",
      "[21, 22]\n",
      "[21, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 22]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize with BERT tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Tokenize with ClinicalBERT tokenizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", do_lower_case=True)\n",
    "# Tokenize with ClinicalBERT tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\", do_lower_case=True)\n",
    "\n",
    "train_mid = [[\"[CLS]\"] + query[:50] + [\"[SEP]\"] for query in train_seq]\n",
    "dev_mid = [[\"[CLS]\"] + query[:50] + [\"[SEP]\"] for query in dev_seq]\n",
    "test_mid = [[\"[CLS]\"] + query[:50] + [\"[SEP]\"] for query in test_seq]\n",
    "train_mid=[[tokenizer.tokenize(w)[0] for w in seq] for seq in train_mid]\n",
    "dev_mid=[[tokenizer.tokenize(w)[0] for w in seq] for seq in dev_mid]\n",
    "test_mid=[[tokenizer.tokenize(w)[0] for w in seq] for seq in test_mid]\n",
    "\n",
    "train_slot_mid = [[integer_mapping_slot['[CLS]']] + list(query) + [integer_mapping_slot['[SEP]']] for query in train_slot_transformed]\n",
    "dev_slot_mid = [[integer_mapping_slot['[CLS]']] + list(query) + [integer_mapping_slot['[SEP]']] for query in dev_slot_transformed]\n",
    "test_slot_mid = [[integer_mapping_slot['[CLS]']] + list(query) + [integer_mapping_slot['[SEP]']] for query in test_slot_transformed]\n",
    "\n",
    "print(train_mid[0])\n",
    "print(dev_mid[0])\n",
    "print(test_mid[0])\n",
    "print(train_slot_mid[0])\n",
    "print(dev_slot_mid[0])\n",
    "print(test_slot_mid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 52\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "# Pad our input tokens\n",
    "train_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in train_mid],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "dev_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in dev_mid],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "test_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in test_mid],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "train_slot_ids = pad_sequences(train_slot_mid,\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\", value=pad_slot_id)\n",
    "\n",
    "dev_slot_ids = pad_sequences(dev_slot_mid,\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\", value=pad_slot_id)\n",
    "test_slot_ids = pad_sequences(test_slot_mid,\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\", value=pad_slot_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "train_attention_masks = []\n",
    "dev_attention_masks = []\n",
    "test_attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in train_input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  train_attention_masks.append(seq_mask)\n",
    "for seq in dev_input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  dev_attention_masks.append(seq_mask)\n",
    "for seq in test_input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  test_attention_masks.append(seq_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_input_ids)\n",
    "dev_inputs = torch.tensor(dev_input_ids)\n",
    "test_inputs = torch.tensor(test_input_ids)\n",
    "\n",
    "train_labels = torch.tensor(train_intent_transformed)\n",
    "dev_labels = torch.tensor(dev_intent_transformed)\n",
    "test_labels = torch.tensor(test_intent_transformed)\n",
    "\n",
    "train_slots = torch.tensor(train_slot_ids)\n",
    "dev_slots = torch.tensor(dev_slot_ids)\n",
    "test_slots = torch.tensor(test_slot_ids)\n",
    "\n",
    "train_masks = torch.tensor(train_attention_masks)\n",
    "dev_masks = torch.tensor(dev_attention_masks)\n",
    "test_masks = torch.tensor(test_attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. \n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader \n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels, train_slots)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "dev_data = TensorDataset(dev_inputs, dev_masks, dev_labels, dev_slots)\n",
    "dev_sampler = SequentialSampler(dev_data)\n",
    "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels, test_slots)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "train_sampler_ = SequentialSampler(train_data)\n",
    "train_dataloader_ = DataLoader(train_data, sampler=train_sampler_, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn import CrossEntropyLoss, MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "class BertForSlotFilling(nn.Module):\n",
    "  def __init__(self, num_labels, ignore_index=None):\n",
    "    super(BertForSlotFilling, self).__init__()\n",
    "    self.num_labels=num_labels\n",
    "    self.ignore_index=ignore_index\n",
    "    # self.bert=BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    # microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\n",
    "    \n",
    "    self.bert=AutoModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\", output_hidden_states=True)\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "    self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "    self._init_weights()\n",
    "\n",
    "\n",
    "  def _init_weights(self):\n",
    "    \"\"\" Initialize the weights \"\"\"\n",
    "    print(\"Initializing linear weight\")\n",
    "    self.classifier.weight.data.normal_(mean=0.0, std=self.bert.config.initializer_range)\n",
    "  \n",
    "  def forward(self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None):\n",
    "    \n",
    "    outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "    sequence_output = outputs[0]\n",
    "    sequence_output = self.dropout(sequence_output)\n",
    "\n",
    "    logits = self.classifier(sequence_output)\n",
    "    outputs = (logits,) + (outputs[0],)\n",
    "\n",
    "    if labels is not None:\n",
    "       loss_fct = nn.CrossEntropyLoss(ignore_index=self.ignore_index)\n",
    "       if attention_mask is not None:\n",
    "          active_loss = attention_mask.view(-1) == 1\n",
    "          active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
    "          active_labels = labels.view(-1)[active_loss]\n",
    "          loss = loss_fct(active_logits, active_labels)\n",
    "       else:\n",
    "          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "       outputs = (loss,) + outputs\n",
    "\n",
    "    return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing linear weight\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSlotFilling(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=23, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "model_slot=BertForSlotFilling(slot_num, pad_slot_id)\n",
    "model_slot.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "# BERT fine-tuning parameters\n",
    "param_optimizer = list(model_slot.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "  \n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "# Number of training epochs \n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 1/10 [02:44<24:40, 164.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09037472169886508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 2/10 [05:29<21:57, 164.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04684897442411132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███       | 3/10 [08:14<19:15, 165.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03635198005617639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 4/10 [10:59<16:29, 164.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.029193883906901256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 5/10 [13:45<13:46, 165.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.02398465307809645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 6/10 [16:30<11:00, 165.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.020532558728856773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|███████   | 7/10 [19:21<08:21, 167.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.017707488238836624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 8/10 [22:11<05:35, 167.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.015768882738400387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|█████████ | 9/10 [24:59<02:47, 167.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.014529930192853953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [27:46<00:00, 166.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.013013862431845135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# BERT training loop\n",
    "for _ in trange(epochs, desc=\"Epoch\"):  \n",
    "  \n",
    "  ## TRAINING\n",
    "  \n",
    "  # Set our model to training mode\n",
    "  model_slot.train()  \n",
    "  # Tracking variables\n",
    "  tr_loss = 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, _, b_slot_labels = batch\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    (loss, _, _, ) = model_slot(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_slot_labels)\n",
    "\n",
    "    train_loss_set.append(loss.item())    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mask(mask_in_batch):\n",
    "  for i,mask in enumerate(mask_in_batch):\n",
    "    sep_index=int(mask.sum()-1)\n",
    "    b_input_mask[i][0]=0\n",
    "    b_input_mask[i][sep_index]=0\n",
    "  return mask_in_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model in evaluation mode\n",
    "model_slot.eval()\n",
    "\n",
    "train_slot_embedding=[]\n",
    "for batch in train_dataloader_:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, _, b_slot_labels = batch\n",
    "\n",
    "  # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "  with torch.no_grad():\n",
    "    # Forward pass, calculate logit predictions\n",
    "    (_, _, hidden_layer, ) = model_slot(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_slot_labels) \n",
    "    dim=hidden_layer.size(-1)\n",
    "    b_input_mask=process_mask(b_input_mask)\n",
    "    b_input_mask=b_input_mask.unsqueeze(-1).repeat(1,1,dim)\n",
    "    embed=hidden_layer*b_input_mask\n",
    "    train_slot_embedding.extend(np.array(embed[:,1:-1,:].cpu()))  \n",
    "\n",
    "\n",
    "dev_slot_embedding=[]\n",
    "for batch in dev_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, _, b_slot_labels = batch\n",
    "  # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "  with torch.no_grad():\n",
    "    # Forward pass, calculate logit predictions\n",
    "    (_, _, hidden_layer, ) = model_slot(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_slot_labels) \n",
    "    dim=hidden_layer.size(-1)\n",
    "    b_input_mask=process_mask(b_input_mask)\n",
    "    b_input_mask=b_input_mask.unsqueeze(-1).repeat(1,1,dim)\n",
    "    embed=hidden_layer*b_input_mask\n",
    "    dev_slot_embedding.extend(np.array(embed[:,1:-1,:].cpu()))  \n",
    "\n",
    "test_slot_embedding=[]\n",
    "for batch in test_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, _, b_slot_labels = batch\n",
    "  # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "  with torch.no_grad():\n",
    "    # Forward pass, calculate logit predictions\n",
    "    (_, _, hidden_layer, ) = model_slot(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_slot_labels) \n",
    "    dim=hidden_layer.size(-1)\n",
    "    b_input_mask=process_mask(b_input_mask)\n",
    "    b_input_mask=b_input_mask.unsqueeze(-1).repeat(1,1,dim)\n",
    "    embed=hidden_layer*b_input_mask\n",
    "    test_slot_embedding.extend(np.array(embed[:,1:-1,:].cpu()))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41496, 50, 768)\n",
      "(4538, 50, 768)\n",
      "(30614, 50, 768)\n"
     ]
    }
   ],
   "source": [
    "train_slot_embedding=np.array(train_slot_embedding)\n",
    "dev_slot_embedding=np.array(dev_slot_embedding)\n",
    "test_slot_embedding=np.array(test_slot_embedding)\n",
    "print(train_slot_embedding.shape)\n",
    "print(dev_slot_embedding.shape)\n",
    "print(test_slot_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(\"Data/PUBMEDBERT/train_pubmedbert_ins_gs.h5\", 'w') as f:\n",
    "  f[\"data\"] = np.array(train_slot_embedding)\n",
    "\n",
    "with h5py.File(\"Data/PUBMEDBERT/dev_pubmedbert_ins_gs.h5\", 'w') as f:\n",
    "  f[\"data\"] = np.array(dev_slot_embedding)\n",
    "\n",
    "with h5py.File(\"Data/PUBMEDBERT/test_pubmedbert_ins_gs.h5\", 'w') as f:\n",
    "  f[\"data\"] = np.array(test_slot_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############1129 end here "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
